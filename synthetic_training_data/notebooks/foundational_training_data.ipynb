{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDW5hmd1VHgR",
        "outputId": "fc182a9b-75fa-4858-f989-3e84ad00dada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 3 parquet files.\n",
            "• test-00001-of-00003.parquet\n",
            "• test-00002-of-00003.parquet\n",
            "• test-00000-of-00003.parquet\n",
            "\n",
            "=== FILE: test-00001-of-00003.parquet ===\n",
            "\n",
            "--- Arrow Schema ---\n",
            "<pyarrow._parquet.ParquetSchema object at 0x7a73cc71e540>\n",
            "required group field_id=-1 schema {\n",
            "  optional group field_id=-1 audio_filepath {\n",
            "    optional binary field_id=-1 bytes;\n",
            "    optional binary field_id=-1 path (String);\n",
            "  }\n",
            "  optional double field_id=-1 duration;\n",
            "  optional binary field_id=-1 text (String);\n",
            "  optional binary field_id=-1 gender (String);\n",
            "  optional binary field_id=-1 age-group (String);\n",
            "  optional binary field_id=-1 primary_language (String);\n",
            "  optional binary field_id=-1 native_place_state (String);\n",
            "  optional binary field_id=-1 native_place_district (String);\n",
            "  optional binary field_id=-1 highest_qualification (String);\n",
            "  optional binary field_id=-1 job_category (String);\n",
            "  optional binary field_id=-1 occupation_domain (String);\n",
            "}\n",
            "\n",
            "\n",
            "--- Columns ---\n",
            "['audio_filepath', 'duration', 'text', 'gender', 'age-group', 'primary_language', 'native_place_state', 'native_place_district', 'highest_qualification', 'job_category', 'occupation_domain']\n",
            "\n",
            "--- dtypes ---\n",
            "audio_filepath            object\n",
            "duration                 float64\n",
            "text                      object\n",
            "gender                    object\n",
            "age-group                 object\n",
            "primary_language          object\n",
            "native_place_state        object\n",
            "native_place_district     object\n",
            "highest_qualification     object\n",
            "job_category              object\n",
            "occupation_domain         object\n",
            "dtype: object\n",
            "\n",
            "--- Sample rows ---\n",
            "{\"audio_filepath\": \"{'bytes': b'RIFF|\\\\x9e\\\\x04\\\\x00WAVEfmt \\\\x10\\\\x00\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x80>\\\\x00\\\\x00\\\\x00}\\\\x00\\\\x00\\\\x02\\\\x00\\\\x10\\\\x00dataX\\\\x9e\\\\x04\\\\x00\\\\xc2\\\\xff\\\\xaa\\\\xff\\\\xd0\\\\xff\\\\xd0\\\\xff\\\\xdb\\\\xff\\\\xeb\\\\xff\\\\x08\\\\x00\\\\r\\\\x00\\\\r\\\\x00(\\\\x0 ...[truncated]...\", \"duration\": \"9.45875\", \"text\": \"some in the starting, then again I poured and finally it happened that it became so wet that to have a\", \"gender\": \"Female\", \"age-group\": \"45-60\", \"primary_language\": \"Maithili\", \"native_place_state\": \"Bihar\", \"native_place_district\": \"Darbhanga\", \"highest_qualification\": \"Graduate\", \"job_category\": \"Full Time\", \"occupation_domain\": \"Education and Research\"}\n",
            "{\"audio_filepath\": \"{'bytes': b'RIFF\\\\x02\\\\xae\\\\x02\\\\x00WAVEfmt \\\\x10\\\\x00\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x80>\\\\x00\\\\x00\\\\x00}\\\\x00\\\\x00\\\\x02\\\\x00\\\\x10\\\\x00data\\\\xde\\\\xad\\\\x02\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x ...[truncated]...\", \"duration\": \"5.4869375\", \"text\": \"North 24 Parganas, South 24 Parganas, Murshidabad, Birbhum, Hooghly.\", \"gender\": \"Female\", \"age-group\": \"18-30\", \"primary_language\": \"Bengali\", \"native_place_state\": \"West Bengal\", \"native_place_district\": \"South 24 Parganas\", \"highest_qualification\": \"Post Graduate\", \"job_category\": \"Full Time\", \"occupation_domain\": \"Information and Media\"}\n",
            "{\"audio_filepath\": \"{'bytes': b'RIFF\\\\xd6\\\\x96\\\\x01\\\\x00WAVEfmt \\\\x10\\\\x00\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x80>\\\\x00\\\\x00\\\\x00}\\\\x00\\\\x00\\\\x02\\\\x00\\\\x10\\\\x00data\\\\xb2\\\\x96\\\\x01\\\\x00y\\\\x01>\\\\x06\\\\xe9\\\\x06\\\\xea\\\\x06\\\\xff\\\\x03\\\\x18\\\\x04J\\\\x07\\\\xb3\\\\x04\\\\xb5\\\\x03*\\\\x ...[truncated]...\", \"duration\": \"3.2535625\", \"text\": \"Breast cancers can be classified by different schemata.\", \"gender\": \"Female\", \"age-group\": \"30-45\", \"primary_language\": \"Konkani\", \"native_place_state\": \"Goa\", \"native_place_district\": \"North Goa\", \"highest_qualification\": \"Post Graduate\", \"job_category\": \"Full Time\", \"occupation_domain\": \"Education and Research\"}\n",
            "{\"audio_filepath\": \"{'bytes': b'RIFF&\\\\x81\\\\x01\\\\x00WAVEfmt \\\\x10\\\\x00\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x80>\\\\x00\\\\x00\\\\x00}\\\\x00\\\\x00\\\\x02\\\\x00\\\\x10\\\\x00data\\\\x02\\\\x81\\\\x01\\\\x00\\\\x1e\\\\x00\\\\x1d\\\\x00\\\\x19\\\\x00\\\\x1e\\\\x00\\\\x11\\\\x00\\\\x0f\\\\x00\\\\x0f\\\\x00\\\\x08\\\\x00\\\\x0b\\\\ ...[truncated]...\", \"duration\": \"3.0800625\", \"text\": \"a railway link between Ahmedabad and Mumbai\", \"gender\": \"Male\", \"age-group\": \"30-45\", \"primary_language\": \"Urdu\", \"native_place_state\": \"Gujarat\", \"native_place_district\": \"Ahmedabad\", \"highest_qualification\": \"Graduate\", \"job_category\": \"Full Time\", \"occupation_domain\": \"Technology and Services\"}\n",
            "{\"audio_filepath\": \"{'bytes': b'RIFF\\\\xb4\\\\x90\\\\x02\\\\x00WAVEfmt \\\\x10\\\\x00\\\\x00\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x80>\\\\x00\\\\x00\\\\x00}\\\\x00\\\\x00\\\\x02\\\\x00\\\\x10\\\\x00data\\\\x90\\\\x90\\\\x02\\\\x00\\\\xfb\\\\xff\\\\x05\\\\x00\\\\x04\\\\x00\\\\x0b\\\\x00\\\\x0c\\\\x00\\\\r\\\\x00\\\\x06\\\\x00\\\\x04\\\\x00\\\\x05 ...[truncated]...\", \"duration\": \"5.2525\", \"text\": \"What to consider before investing in Employee Provident Fund Savings Scheme?\", \"gender\": \"Female\", \"age-group\": \"30-45\", \"primary_language\": \"Kashmiri\", \"native_place_state\": \"Jammu Kashmir\", \"native_place_district\": \"Srinagar\", \"highest_qualification\": \"Doctoral (PhD) or higher level\", \"job_category\": \"Other\", \"occupation_domain\": \"Education and Research\"}\n"
          ]
        }
      ],
      "source": [
        "# --- Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Point to your Parquet folder (adjust if needed)\n",
        "PARQUET_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/english\"\n",
        "\n",
        "import os, json, itertools\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "\n",
        "# Find parquet files\n",
        "parquet_files = [os.path.join(PARQUET_DIR, f) for f in os.listdir(PARQUET_DIR) if f.endswith(\".parquet\")]\n",
        "assert parquet_files, f\"No .parquet files found in {PARQUET_DIR}\"\n",
        "print(f\"Found {len(parquet_files)} parquet files.\")\n",
        "for f in parquet_files[:5]:\n",
        "    print(\"•\", os.path.basename(f))\n",
        "\n",
        "def show_schema_and_samples(pq_path, n=5):\n",
        "    print(\"\\n=== FILE:\", os.path.basename(pq_path), \"===\")\n",
        "    pf = pq.ParquetFile(pq_path)\n",
        "    print(\"\\n--- Arrow Schema ---\")\n",
        "    print(pf.schema)\n",
        "\n",
        "    # Read first n rows as pandas for easy viewing\n",
        "    df = pf.read_row_groups([0], columns=None).to_pandas()\n",
        "    print(\"\\n--- Columns ---\")\n",
        "    print(list(df.columns))\n",
        "    print(\"\\n--- dtypes ---\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    # Show a few rows (truncate long fields)\n",
        "    def trunc(x):\n",
        "        s = str(x)\n",
        "        return s if len(s) < 200 else s[:200] + \" ...[truncated]...\"\n",
        "    print(\"\\n--- Sample rows ---\")\n",
        "    for i in range(min(n, len(df))):\n",
        "        row = {c: trunc(df.iloc[i][c]) for c in df.columns}\n",
        "        print(json.dumps(row, ensure_ascii=False))\n",
        "\n",
        "# Probe first file (repeat if partitions differ)\n",
        "show_schema_and_samples(parquet_files[0], n=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile pyarrow pandas\n",
        "\n",
        "import os, json, hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import io\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Paths\n",
        "PARQUET_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/english\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/english_wav\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "manifest_rows = []\n",
        "\n",
        "# Iterate over all parquet files\n",
        "parquet_files = [os.path.join(PARQUET_DIR, f) for f in os.listdir(PARQUET_DIR) if f.endswith(\".parquet\")]\n",
        "for fidx, pq_path in enumerate(parquet_files):\n",
        "    print(f\"\\nProcessing {fidx+1}/{len(parquet_files)}: {os.path.basename(pq_path)}\")\n",
        "    df = pd.read_parquet(pq_path)\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        try:\n",
        "            # audio_filepath is a dict with \"bytes\" (raw WAV)\n",
        "            audio_dict = row[\"audio_filepath\"]\n",
        "            if not isinstance(audio_dict, dict) or \"bytes\" not in audio_dict:\n",
        "                continue\n",
        "\n",
        "            wav_bytes = audio_dict[\"bytes\"]\n",
        "            # Decode WAV bytes\n",
        "            audio_data, sr = sf.read(io.BytesIO(wav_bytes), dtype=\"float32\", always_2d=False)\n",
        "\n",
        "            # Flatten stereo to mono if needed\n",
        "            if audio_data.ndim == 2:\n",
        "                audio_data = audio_data.mean(axis=1)\n",
        "\n",
        "            # Build a deterministic filename\n",
        "            uid = hashlib.md5(f\"{os.path.basename(pq_path)}::{i}\".encode(\"utf-8\")).hexdigest()[:12]\n",
        "            out_wav = os.path.join(OUT_DIR, f\"iitm_{uid}.wav\")\n",
        "\n",
        "            # Save .wav\n",
        "            sf.write(out_wav, audio_data, sr)\n",
        "\n",
        "            # Collect metadata for manifest\n",
        "            manifest_rows.append({\n",
        "                \"audio_file\": out_wav,\n",
        "                \"duration\": row.get(\"duration\", \"\"),\n",
        "                \"transcript\": row.get(\"text\", \"\"),\n",
        "                \"gender\": row.get(\"gender\", \"\"),\n",
        "                \"age_group\": row.get(\"age-group\", \"\"),\n",
        "                \"primary_language\": row.get(\"primary_language\", \"\"),\n",
        "                \"native_place_state\": row.get(\"native_place_state\", \"\"),\n",
        "                \"native_place_district\": row.get(\"native_place_district\", \"\"),\n",
        "                \"highest_qualification\": row.get(\"highest_qualification\", \"\"),\n",
        "                \"job_category\": row.get(\"job_category\", \"\"),\n",
        "                \"occupation_domain\": row.get(\"occupation_domain\", \"\")\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ! Error row {i}: {e}\")\n",
        "\n",
        "# Save manifest CSV\n",
        "manifest_path = os.path.join(OUT_DIR, \"manifest.csv\")\n",
        "pd.DataFrame(manifest_rows).to_csv(manifest_path, index=False, encoding=\"utf-8\")\n",
        "print(\"\\n✅ Done. Wrote\", len(manifest_rows), \"rows to\", manifest_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKHTGnz9VOOq",
        "outputId": "7a0c2914-c354-4f0a-e0a8-b33f8693b9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "Processing 1/3: test-00001-of-00003.parquet\n",
            "\n",
            "Processing 2/3: test-00002-of-00003.parquet\n",
            "\n",
            "Processing 3/3: test-00000-of-00003.parquet\n",
            "\n",
            "✅ Done. Wrote 6656 rows to /content/drive/MyDrive/problem_statement_6/training_data/english_wav/manifest.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "pa_dir = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN\"\n",
        "hi_dir = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi\"\n",
        "\n",
        "print(\"Punjabi folder contents:\")\n",
        "print(os.listdir(pa_dir))\n",
        "\n",
        "print(\"\\nHindi folder contents:\")\n",
        "print(os.listdir(hi_dir))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkg8vS-qXzBf",
        "outputId": "9ad9dcb4-b4f5-4306-a15d-cc4ca4fc38a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punjabi folder contents:\n",
            "['test_manifest.json', 'clips_16k']\n",
            "\n",
            "Hindi folder contents:\n",
            "['test_manifest.json', 'clips_16k']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Config: set your two folders\n",
        "PA_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN\"\n",
        "HI_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi\"\n",
        "\n",
        "import os, json, pandas as pd\n",
        "\n",
        "def peek_manifest(manifest_path, n=5):\n",
        "    print(f\"\\nPreview: {manifest_path}\")\n",
        "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= n: break\n",
        "            print(line.strip())\n",
        "\n",
        "def load_manifest_to_rows(lang_dir, lang_label, manifest_basename=\"test_manifest.json\"):\n",
        "    \"\"\"\n",
        "    Reads JSON-lines manifest and returns a list of rows with absolute audio paths.\n",
        "    Expected keys (common in CV prepared exports):\n",
        "      - audio_filepath (relative path like 'clips_16k/xxx.wav')\n",
        "      - duration (seconds)\n",
        "      - text (transcript)\n",
        "      - optional: client_id, gender, age, accent, locale\n",
        "    \"\"\"\n",
        "    mpath = os.path.join(lang_dir, manifest_basename)\n",
        "    assert os.path.exists(mpath), f\"Manifest not found: {mpath}\"\n",
        "\n",
        "    rows, missing = [], 0\n",
        "    with open(mpath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            rel = rec.get(\"audio_filepath\") or rec.get(\"path\")  # fallback key\n",
        "            if not rel:\n",
        "                continue\n",
        "            # Build absolute path\n",
        "            wav_abs = os.path.join(lang_dir, rel)\n",
        "            if not os.path.isabs(wav_abs):\n",
        "                wav_abs = os.path.abspath(wav_abs)\n",
        "            exists = os.path.exists(wav_abs)\n",
        "            if not exists:\n",
        "                missing += 1\n",
        "\n",
        "            rows.append({\n",
        "                \"audio_file\": wav_abs,\n",
        "                \"exists\": exists,\n",
        "                \"duration\": rec.get(\"duration\", \"\"),\n",
        "                \"transcript\": rec.get(\"text\", \"\"),\n",
        "                \"language\": lang_label,\n",
        "                \"client_id\": rec.get(\"client_id\", \"\"),\n",
        "                \"gender\": rec.get(\"gender\", \"\"),\n",
        "                \"age\": rec.get(\"age\", \"\"),\n",
        "                \"accent\": rec.get(\"accent\", \"\"),\n",
        "                \"locale\": rec.get(\"locale\", \"\"),\n",
        "                \"source\": f\"common_voice_11:{lang_label}\"\n",
        "            })\n",
        "    print(f\"{lang_label}: parsed {len(rows)} rows | missing files: {missing}\")\n",
        "    return rows\n",
        "\n",
        "# 1) Peek the first few lines to confirm schema\n",
        "peek_manifest(os.path.join(PA_DIR, \"test_manifest.json\"))\n",
        "peek_manifest(os.path.join(HI_DIR, \"test_manifest.json\"))\n",
        "\n",
        "# 2) Convert to CSV manifests\n",
        "pa_rows = load_manifest_to_rows(PA_DIR, lang_label=\"punjabi\", manifest_basename=\"test_manifest.json\")\n",
        "hi_rows = load_manifest_to_rows(HI_DIR, lang_label=\"hindi\",   manifest_basename=\"test_manifest.json\")\n",
        "\n",
        "pa_df = pd.DataFrame(pa_rows)\n",
        "hi_df = pd.DataFrame(hi_rows)\n",
        "\n",
        "pa_csv = os.path.join(PA_DIR, \"manifest_test.csv\")\n",
        "hi_csv = os.path.join(HI_DIR, \"manifest_test.csv\")\n",
        "\n",
        "# Keep only rows whose audio exists\n",
        "pa_df = pa_df[pa_df[\"exists\"]].drop(columns=[\"exists\"])\n",
        "hi_df = hi_df[hi_df[\"exists\"]].drop(columns=[\"exists\"])\n",
        "\n",
        "pa_df.to_csv(pa_csv, index=False, encoding=\"utf-8\")\n",
        "hi_df.to_csv(hi_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n✅ Wrote:\")\n",
        "print(\" -\", pa_csv, f\"({len(pa_df)} rows)\")\n",
        "print(\" -\", hi_csv, f\"({len(hi_df)} rows)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxqmgWBfAjcp",
        "outputId": "c63a4e8b-88bb-4e9d-9509-998e3901f749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preview: /content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/test_manifest.json\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/clips_16k/common_voice_pa-IN_25183731.wav\", \"duration\": 3.924, \"text\": \"\\u0a26\\u0a47\\u0a16\\u0a26\\u0a47 \\u0a26\\u0a47\\u0a16\\u0a26\\u0a47 \\u0a35\\u0a3f\\u0a39\\u0a5c\\u0a3e \\u0a32\\u0a4b\\u0a15\\u0a3e\\u0a02 \\u0a28\\u0a3e\\u0a32 \\u0a2d\\u0a30 \\u0a17\\u0a3f\\u0a06\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/clips_16k/common_voice_pa-IN_24666519.wav\", \"duration\": 3.96, \"text\": \"\\u0a2a\\u0a3e\\u0a23\\u0a40 \\u0a09\\u0a2c\\u0a3e\\u0a32 \\u0a15\\u0a47 \\u0a39\\u0a40 \\u0a2a\\u0a40\\u0a23\\u0a3e \\u0a1a\\u0a3e\\u0a39\\u0a40\\u0a26\\u0a3e \\u0a39\\u0a48\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/clips_16k/common_voice_pa-IN_24666521.wav\", \"duration\": 4.608, \"text\": \"\\u0a38\\u0a3e\\u0a30\\u0a3e \\u0a2a\\u0a3e\\u0a32\\u0a3f\\u0a28 \\u0a39\\u0a4b\\u0a08 \\u0a16\\u0a41\\u0a2c\\u0a38\\u0a42\\u0a30\\u0a24 \\u0a38\\u0a3c\\u0a30\\u0a3e\\u0a30\\u0a24 \\u0a26\\u0a3e \\u0a38\\u0a3f\\u0a3c\\u0a15\\u0a3e\\u0a30\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/clips_16k/common_voice_pa-IN_24974796.wav\", \"duration\": 3.528, \"text\": \"\\u0a06\\u0a16\\u0a30 \\u0a38\\u0a3e\\u0a21\\u0a40 \\u0a1c\\u0a3c\\u0a3f\\u0a70\\u0a26\\u0a17\\u0a40 \\u0a26\\u0a3e \\u0a1f\\u0a40\\u0a1a\\u0a3e \\u0a15\\u0a40 \\u0a39\\u0a48\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/clips_16k/common_voice_pa-IN_24974891.wav\", \"duration\": 3.06, \"text\": \"\\u0a07\\u0a15 \\u0a38\\u0a48\\u0a2e\\u0a40\\u0a2b\\u0a3e\\u0a08\\u0a28\\u0a32 \\u0a2e\\u0a41\\u0a15\\u0a3e\\u0a2c\\u0a32\\u0a3e \\u0a35\\u0a40 \\u0a38\\u0a3c\\u0a3e\\u0a2e\\u0a32 \\u0a39\\u0a48\"}\n",
            "\n",
            "Preview: /content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi/test_manifest.json\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/hi/clips_16k/common_voice_hi_31389203.wav\", \"duration\": 5.436, \"text\": \"\\u0935\\u093e\\u0930\\u093e\\u0923\\u0938\\u0940 \\u0939\\u093f\\u0902\\u0938\\u093e \\u092a\\u0941\\u0932\\u093f\\u0938 \\u0915\\u0940 \\u092d\\u0942\\u092e\\u093f\\u0915\\u093e \\u0938\\u0902\\u0926\\u093f\\u0917\\u094d\\u0927 \\u0935\\u093e\\u092f\\u0930\\u0932 \\u0939\\u0941\\u0908 \\u0924\\u0938\\u094d\\u0935\\u0940\\u0930\\u0947\\u0902\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/hi/clips_16k/common_voice_hi_27408237.wav\", \"duration\": 5.58, \"text\": \"Box office \\u0915\\u093e \\u0928\\u092f\\u093e \\u0939\\u0940\\u0930\\u094b \\u092c\\u0928\\u093e \\u0939\\u0949\\u0930\\u0930 \\u0932\\u0948\\u0932\\u093e \\u092e\\u091c\\u0928\\u0942 \\u092a\\u0932\\u091f\\u0928 \\u092a\\u0940\\u091b\\u0947\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/hi/clips_16k/common_voice_hi_28746190.wav\", \"duration\": 5.724, \"text\": \"\\u091b\\u0947\\u0921\\u093c\\u091b\\u093e\\u0921\\u093c \\u0915\\u093e \\u0935\\u093f\\u0930\\u094b\\u0927 \\u0915\\u0930 \\u0930\\u0939\\u0947 \\u091b\\u093e\\u0924\\u094d\\u0930\\u094b\\u0902 \\u0915\\u094b \\u092a\\u0941\\u0932\\u093f\\u0938 \\u0928\\u0947 \\u092a\\u0940\\u091f\\u093e \\u0914\\u0930 \\u091c\\u0947\\u0932 \\u092e\\u0947\\u0902 \\u0921\\u093e\\u0932\\u093e\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/hi/clips_16k/common_voice_hi_25248770.wav\", \"duration\": 3.06, \"text\": \"\\u0924\\u0941\\u092e\\u094d\\u0939\\u093e\\u0930\\u0947 \\u092c\\u0939\\u0941\\u0924 \\u0938\\u093e\\u0930\\u0947 \\u0926\\u0941\\u0936\\u092e\\u0928 \\u0939\\u0948\\u0902\"}\n",
            "{\"audio_filepath\": \"/nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/asr_datasets/commonvoice/cv-corpus-11.0-2022-09-21/hi/clips_16k/common_voice_hi_31803311.wav\", \"duration\": 9.648, \"text\": \"\\u092f\\u0942\\u092a\\u0940 \\u092f\\u094b\\u0917\\u0940 \\u0938\\u0930\\u0915\\u093e\\u0930 \\u0915\\u0947 \\u0916\\u093f\\u0932\\u093e\\u092b \\u092c\\u0940\\u091c\\u0947\\u092a\\u0940 \\u0935\\u093f\\u0927\\u093e\\u092f\\u0915\\u094b\\u0902 \\u0915\\u093e \\u0927\\u0930\\u0928\\u093e \\u0916\\u0924\\u094d\\u092e \\u0932\\u0947\\u0915\\u093f\\u0928 \\u0926\\u0940 \\u092f\\u0947 \\u091a\\u0947\\u0924\\u093e\\u0935\\u0928\\u0940\"}\n",
            "punjabi: parsed 171 rows | missing files: 171\n",
            "hindi: parsed 1727 rows | missing files: 1727\n",
            "\n",
            "✅ Wrote:\n",
            " - /content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/manifest_test.csv (0 rows)\n",
            " - /content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi/manifest_test.csv (0 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pandas as pd\n",
        "\n",
        "# Folders you showed\n",
        "PA_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN\"\n",
        "HI_DIR = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi\"\n",
        "\n",
        "def fix_manifest(lang_dir, lang_label, in_name=\"test_manifest.json\", out_name=\"manifest_test.csv\"):\n",
        "    manifest_path = os.path.join(lang_dir, in_name)\n",
        "    clips_dir = os.path.join(lang_dir, \"clips_16k\")\n",
        "\n",
        "    # Index local files by basename for fast lookup\n",
        "    local_files = {}\n",
        "    for fn in os.listdir(clips_dir):\n",
        "        if fn.lower().endswith((\".wav\", \".mp3\", \".flac\", \".ogg\")):\n",
        "            local_files[fn] = os.path.join(clips_dir, fn)\n",
        "\n",
        "    rows, missing = [], []\n",
        "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            rec = json.loads(line)\n",
        "            # Get the remote path and collapse to basename\n",
        "            remote = rec.get(\"audio_filepath\") or rec.get(\"path\")\n",
        "            if not remote:\n",
        "                continue\n",
        "            base = os.path.basename(remote)  # e.g., common_voice_hi_31389203.wav\n",
        "\n",
        "            # Map to local path\n",
        "            wav_abs = local_files.get(base)\n",
        "            if not wav_abs:\n",
        "                missing.append(base)\n",
        "                continue\n",
        "\n",
        "            rows.append({\n",
        "                \"audio_file\": wav_abs,\n",
        "                \"duration\": rec.get(\"duration\", \"\"),\n",
        "                \"transcript\": rec.get(\"text\", \"\").strip(),\n",
        "                \"language\": lang_label,\n",
        "                \"client_id\": rec.get(\"client_id\", \"\"),\n",
        "                \"gender\": rec.get(\"gender\", \"\"),\n",
        "                \"age\": rec.get(\"age\", \"\"),\n",
        "                \"accent\": rec.get(\"accent\", \"\"),\n",
        "                \"locale\": rec.get(\"locale\", \"\"),\n",
        "                \"source\": f\"common_voice_11:{lang_label}\"\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    out_csv = os.path.join(lang_dir, out_name)\n",
        "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"{lang_label}: kept {len(df)} rows; missing matches: {len(missing)}\")\n",
        "    if missing[:5]:\n",
        "        print(\"  sample missing basenames:\", missing[:5])\n",
        "    print(\"  wrote:\", out_csv)\n",
        "    return df\n",
        "\n",
        "pa_df = fix_manifest(PA_DIR, \"punjabi\")\n",
        "hi_df = fix_manifest(HI_DIR, \"hindi\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS6L2t3kF5IK",
        "outputId": "4e600e7e-3d76-48b5-fc20-aa92a6bab2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punjabi: kept 171 rows; missing matches: 0\n",
            "  wrote: /content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/manifest_test.csv\n",
            "hindi: kept 1727 rows; missing matches: 0\n",
            "  wrote: /content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi/manifest_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pandas as pd\n",
        "\n",
        "# Paths (adjust english_manifest if your IIT-M export lives elsewhere)\n",
        "pa_csv = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/pa-IN/manifest_test.csv\"\n",
        "hi_csv = \"/content/drive/MyDrive/problem_statement_6/training_data/commonvoice/commonvoice/cv-corpus-11.0-2022-09-21/hi/manifest_test.csv\"\n",
        "english_manifest = \"/content/drive/MyDrive/problem_statement_6/training_data/english_wav/manifest.csv\"  # from your IIT-M parquet conversion step\n",
        "out_merged = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "\n",
        "dfs = []\n",
        "for p in [english_manifest, hi_csv, pa_csv]:\n",
        "    if not os.path.exists(p):\n",
        "        print(\"⚠️ Missing:\", p)\n",
        "        continue\n",
        "    df = pd.read_csv(p)\n",
        "    # Harmonize columns\n",
        "    rename_map = {\n",
        "        \"text\":\"transcript\",\n",
        "        \"age-group\":\"speaker_age_group\",\n",
        "        \"age\":\"speaker_age_group\",\n",
        "        \"gender\":\"speaker_gender\"\n",
        "    }\n",
        "    df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
        "    # Ensure required fields\n",
        "    for c in [\"audio_file\",\"transcript\",\"language\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\"\n",
        "    # Backfill language if absent\n",
        "    if (df[\"language\"] == \"\").any():\n",
        "        # infer from path when possible\n",
        "        df.loc[df[\"language\"]==\"\", \"language\"] = df[\"audio_file\"].str.extract(r\"/(hi|pa-IN)/\", expand=False).map({\"hi\":\"hindi\",\"pa-IN\":\"punjabi\"})\n",
        "        # default English for IIT-M if still empty\n",
        "        df.loc[(df[\"language\"]==\"\"), \"language\"] = \"english\"\n",
        "    # Strip whitespace\n",
        "    df[\"transcript\"] = df[\"transcript\"].fillna(\"\").astype(str).str.strip()\n",
        "    df[\"audio_file\"] = df[\"audio_file\"].astype(str)\n",
        "    # Keep only rows with real files and non-empty transcript\n",
        "    df = df[df[\"audio_file\"].apply(os.path.exists)]\n",
        "    df = df[df[\"transcript\"].str.len() > 0]\n",
        "    dfs.append(df)\n",
        "\n",
        "assert dfs, \"No input manifests found. Check the paths.\"\n",
        "\n",
        "full = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Optional: drop dupes by identical (audio_file, transcript)\n",
        "full = full.drop_duplicates(subset=[\"audio_file\",\"transcript\"])\n",
        "\n",
        "# Tidy language labels\n",
        "full[\"language\"] = full[\"language\"].str.lower().replace({\n",
        "    \"en\":\"english\",\"eng\":\"english\",\n",
        "    \"hi\":\"hindi\",\"hin\":\"hindi\",\n",
        "    \"pa\":\"punjabi\",\"pa-in\":\"punjabi\",\"pan\":\"punjabi\"\n",
        "})\n",
        "\n",
        "# Save\n",
        "full.to_csv(out_merged, index=False, encoding=\"utf-8\")\n",
        "print(f\"✅ Saved unified manifest: {out_merged}\")\n",
        "print(\"Rows:\", len(full))\n",
        "\n",
        "# Quick stats\n",
        "print(\"\\n— Language counts —\")\n",
        "print(full[\"language\"].value_counts())\n",
        "\n",
        "if \"speaker_gender\" in full.columns:\n",
        "    print(\"\\n— Gender (non-empty) —\")\n",
        "    print(full[\"speaker_gender\"].replace({\"\":None}).dropna().value_counts().head())\n",
        "\n",
        "if \"speaker_age_group\" in full.columns:\n",
        "    print(\"\\n— Age group (non-empty) —\")\n",
        "    print(full[\"speaker_age_group\"].replace({\"\":None}).dropna().value_counts().head())\n",
        "\n",
        "# Peek a few rows\n",
        "print(\"\\n— Sample rows —\")\n",
        "print(full[[\"audio_file\",\"language\",\"transcript\"]].head(5).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mf6EwFWGvM3",
        "outputId": "2a1c9d45-f981-4343-a080-685dfb2451b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved unified manifest: /content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\n",
            "Rows: 8554\n",
            "\n",
            "— Language counts —\n",
            "language\n",
            "hindi      1727\n",
            "punjabi     171\n",
            "Name: count, dtype: int64\n",
            "\n",
            "— Gender (non-empty) —\n",
            "speaker_gender\n",
            "Female    3579\n",
            "Male      3077\n",
            "Name: count, dtype: int64\n",
            "\n",
            "— Age group (non-empty) —\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "— Sample rows —\n",
            "                                                                                audio_file language                                                                                             transcript\n",
            "/content/drive/MyDrive/problem_statement_6/training_data/english_wav/iitm_9aafcf3bab2d.wav      NaN some in the starting, then again I poured and finally it happened that it became so wet that to have a\n",
            "/content/drive/MyDrive/problem_statement_6/training_data/english_wav/iitm_bc3b8ad955ee.wav      NaN                                   North 24 Parganas, South 24 Parganas, Murshidabad, Birbhum, Hooghly.\n",
            "/content/drive/MyDrive/problem_statement_6/training_data/english_wav/iitm_949365041e35.wav      NaN                                                Breast cancers can be classified by different schemata.\n",
            "/content/drive/MyDrive/problem_statement_6/training_data/english_wav/iitm_bd8e39b72d07.wav      NaN                                                            a railway link between Ahmedabad and Mumbai\n",
            "/content/drive/MyDrive/problem_statement_6/training_data/english_wav/iitm_60ada8cf9928.wav      NaN                           What to consider before investing in Employee Provident Fund Savings Scheme?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "eng_dir = \"/content/drive/MyDrive/problem_statement_6/training_data/english_wav\"\n",
        "\n",
        "if not os.path.exists(eng_dir):\n",
        "    print(\"⚠️ English folder not found:\", eng_dir)\n",
        "else:\n",
        "    files = os.listdir(eng_dir)\n",
        "    print(f\"English folder has {len(files)} entries\")\n",
        "    # Show a sample of what’s inside\n",
        "    for f in files[:20]:\n",
        "        print(\" •\", f)\n",
        "    # Check if a manifest is present\n",
        "    has_manifest = [f for f in files if f.lower().startswith(\"manifest\") and f.endswith(\".csv\")]\n",
        "    if has_manifest:\n",
        "        print(\"\\n✅ Found manifest(s):\", has_manifest)\n",
        "    else:\n",
        "        print(\"\\n⚠️ No manifest CSV found in english_wav/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye0DesxPG_1z",
        "outputId": "3bd98667-2402-4353-cb53-f9f2d7085b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English folder has 6657 entries\n",
            " • iitm_69aa211ed1cb.wav\n",
            " • iitm_4fd8444d89f5.wav\n",
            " • iitm_8500b9c8f3fa.wav\n",
            " • iitm_bcffc123b4a8.wav\n",
            " • iitm_5cf0cb61e89e.wav\n",
            " • iitm_670e3c6f232d.wav\n",
            " • iitm_2a5c402e12a9.wav\n",
            " • iitm_6050c01ac604.wav\n",
            " • iitm_d2333bdb39fe.wav\n",
            " • iitm_ec1815aa399d.wav\n",
            " • iitm_39c5815f9616.wav\n",
            " • iitm_e8f2b2ba9f17.wav\n",
            " • iitm_b21e95467404.wav\n",
            " • iitm_a313cfd0fc84.wav\n",
            " • iitm_2a4a25b16a75.wav\n",
            " • iitm_70224a33a7cf.wav\n",
            " • iitm_89ef1863fc77.wav\n",
            " • iitm_dd257c8e6087.wav\n",
            " • iitm_c9a96975976d.wav\n",
            " • iitm_71662b714dcc.wav\n",
            "\n",
            "✅ Found manifest(s): ['manifest.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pandas as pd\n",
        "\n",
        "merged_path = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "en_manifest = \"/content/drive/MyDrive/problem_statement_6/training_data/english_wav/manifest.csv\"\n",
        "\n",
        "# Load\n",
        "full = pd.read_csv(merged_path)\n",
        "en   = pd.read_csv(en_manifest)\n",
        "\n",
        "# Harmonize column names\n",
        "en = en.rename(columns={\n",
        "    \"text\": \"transcript\",\n",
        "    \"age-group\": \"speaker_age_group\",\n",
        "    \"gender\": \"speaker_gender\"\n",
        "})\n",
        "\n",
        "# Ensure required columns exist\n",
        "for c in [\"audio_file\",\"transcript\",\"language\"]:\n",
        "    if c not in en.columns:\n",
        "        en[c] = \"\"\n",
        "# Backfill language (English set)\n",
        "en.loc[(en[\"language\"]==\"\") | (en[\"language\"].isna()), \"language\"] = \"english\"\n",
        "\n",
        "# Keep valid rows only\n",
        "en[\"transcript\"] = en[\"transcript\"].fillna(\"\").astype(str).str.strip()\n",
        "en = en[(en[\"audio_file\"].apply(os.path.exists)) & (en[\"transcript\"].str.len() > 0)]\n",
        "\n",
        "# Merge & tidy\n",
        "out = pd.concat([full, en], ignore_index=True).drop_duplicates(subset=[\"audio_file\",\"transcript\"])\n",
        "out[\"language\"] = out[\"language\"].str.lower().replace({\n",
        "    \"en\":\"english\",\"eng\":\"english\",\n",
        "    \"hi\":\"hindi\",\"hin\":\"hindi\",\n",
        "    \"pa\":\"punjabi\",\"pa-in\":\"punjabi\",\"pan\":\"punjabi\"\n",
        "})\n",
        "\n",
        "# Save\n",
        "out.to_csv(merged_path, index=False, encoding=\"utf-8\")\n",
        "print(\"✅ Updated unified manifest:\", merged_path)\n",
        "print(\"Rows:\", len(out))\n",
        "print(\"\\n— Language counts —\")\n",
        "print(out[\"language\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGTaXc8vJF2T",
        "outputId": "0f7c6a77-1106-4a2b-8605-e29fa1bf6eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated unified manifest: /content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\n",
            "Rows: 8554\n",
            "\n",
            "— Language counts —\n",
            "language\n",
            "hindi      1727\n",
            "punjabi     171\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "manifest_path = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "df = pd.read_csv(manifest_path)\n",
        "\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(\"Total rows:\", len(df))\n",
        "\n",
        "# Unique values in language column\n",
        "print(\"\\nUnique values in 'language' column (first 20):\")\n",
        "print(df[\"language\"].dropna().unique()[:20])\n",
        "\n",
        "# Counts including NaN/blank\n",
        "print(\"\\nValue counts (raw):\")\n",
        "print(df[\"language\"].value_counts(dropna=False))\n",
        "\n",
        "# Peek a few rows that are missing language\n",
        "print(\"\\nSample rows where language is blank or NaN:\")\n",
        "print(df[df[\"language\"].isna() | (df[\"language\"].astype(str).str.strip()==\"\")].head(10)[[\"audio_file\",\"language\",\"transcript\"]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6pzVqkwJmN5",
        "outputId": "4dad19bf-88c7-4037-cd3f-8fe877ab8ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['audio_file', 'duration', 'transcript', 'speaker_gender', 'age_group', 'primary_language', 'native_place_state', 'native_place_district', 'highest_qualification', 'job_category', 'occupation_domain', 'language', 'client_id', 'speaker_age_group', 'accent', 'locale', 'source']\n",
            "Total rows: 8554\n",
            "\n",
            "Unique values in 'language' column (first 20):\n",
            "['hindi' 'punjabi']\n",
            "\n",
            "Value counts (raw):\n",
            "language\n",
            "NaN        6656\n",
            "hindi      1727\n",
            "punjabi     171\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample rows where language is blank or NaN:\n",
            "                                          audio_file language  \\\n",
            "0  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "1  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "2  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "3  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "4  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "5  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "6  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "7  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "8  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "9  /content/drive/MyDrive/problem_statement_6/tra...      NaN   \n",
            "\n",
            "                                          transcript  \n",
            "0  some in the starting, then again I poured and ...  \n",
            "1  North 24 Parganas, South 24 Parganas, Murshida...  \n",
            "2  Breast cancers can be classified by different ...  \n",
            "3        a railway link between Ahmedabad and Mumbai  \n",
            "4  What to consider before investing in Employee ...  \n",
            "5  So dancing and singing begins at the headman's...  \n",
            "6   In addition to 1 kg One Good Plant based Nuggets  \n",
            "7  That means we don't have to be panic in troubl...  \n",
            "8                                               Zero  \n",
            "9                                                Two  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "manifest_path = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "df = pd.read_csv(manifest_path)\n",
        "\n",
        "print(\"Before fix:\", df[\"language\"].value_counts(dropna=False))\n",
        "\n",
        "# Fill NaN or blank with \"english\"\n",
        "df[\"language\"] = df[\"language\"].fillna(\"\").astype(str).str.strip().replace({\"\": \"english\"})\n",
        "\n",
        "# Normalize to lowercase\n",
        "df[\"language\"] = df[\"language\"].str.lower()\n",
        "\n",
        "df.to_csv(manifest_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\nAfter fix:\", df[\"language\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RhKDemFj5bA",
        "outputId": "ea96e439-b427-4eea-d017-51f29d7d5566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before fix: language\n",
            "NaN        6656\n",
            "hindi      1727\n",
            "punjabi     171\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After fix: language\n",
            "english    6656\n",
            "hindi      1727\n",
            "punjabi     171\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Duration stats for foundational_manifest.csv\n",
        "\n",
        "!pip -q install soundfile\n",
        "import os, pandas as pd, numpy as np, soundfile as sf\n",
        "from tqdm import tqdm\n",
        "\n",
        "MANIFEST = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "\n",
        "df = pd.read_csv(MANIFEST)\n",
        "assert {\"audio_file\",\"language\",\"transcript\"}.issubset(df.columns), \"manifest is missing required columns\"\n",
        "\n",
        "# --- Ensure a numeric duration column ---\n",
        "if \"duration\" not in df.columns:\n",
        "    df[\"duration\"] = np.nan\n",
        "\n",
        "# Only compute for rows where duration is missing/invalid\n",
        "mask_need = df[\"duration\"].isna() | (df[\"duration\"] <= 0)\n",
        "print(f\"Rows needing duration compute: {mask_need.sum()} / {len(df)}\")\n",
        "\n",
        "def get_duration_sec(path):\n",
        "    try:\n",
        "        info = sf.info(path)\n",
        "        return float(info.frames) / float(info.samplerate)\n",
        "    except Exception:\n",
        "        return np.nan  # keep as NaN if unreadable (bad file, missing, etc.)\n",
        "\n",
        "if mask_need.any():\n",
        "    for idx in tqdm(df[mask_need].index, desc=\"Computing durations\"):\n",
        "        p = df.at[idx, \"audio_file\"]\n",
        "        df.at[idx, \"duration\"] = get_duration_sec(p)\n",
        "\n",
        "# Drop rows with missing files or zero/NaN duration/transcript\n",
        "before = len(df)\n",
        "df = df[df[\"audio_file\"].apply(os.path.exists)]\n",
        "df = df[df[\"transcript\"].fillna(\"\").astype(str).str.strip().ne(\"\")]\n",
        "df = df[df[\"duration\"].notna() & (df[\"duration\"] > 0)]\n",
        "after = len(df)\n",
        "print(f\"Filtered invalid rows: {before - after} removed, {after} remain\")\n",
        "\n",
        "# --- Aggregate stats ---\n",
        "df[\"language\"] = df[\"language\"].astype(str).str.strip().str.lower()\n",
        "counts = df[\"language\"].value_counts().sort_index()\n",
        "hours  = (df.groupby(\"language\")[\"duration\"].sum() / 3600.0).sort_index().round(2)\n",
        "mean_s = df.groupby(\"language\")[\"duration\"].mean().round(2)\n",
        "med_s  = df.groupby(\"language\")[\"duration\"].median().round(2)\n",
        "p95_s  = df.groupby(\"language\")[\"duration\"].quantile(0.95).round(2)\n",
        "\n",
        "print(\"\\n=== Clips per language ===\")\n",
        "print(counts)\n",
        "print(\"\\n=== Hours per language ===\")\n",
        "print(hours)\n",
        "print(\"\\n=== Duration stats (seconds) ===\")\n",
        "stats = pd.DataFrame({\"mean\": mean_s, \"median\": med_s, \"p95\": p95_s})\n",
        "print(stats)\n",
        "\n",
        "# Totals\n",
        "print(\"\\n=== Totals ===\")\n",
        "print(f\"Total clips: {int(counts.sum())}\")\n",
        "print(f\"Total hours: {hours.sum():.2f}\")\n",
        "\n",
        "# (Optional) Save the manifest back with filled durations\n",
        "df.to_csv(MANIFEST, index=False, encoding=\"utf-8\")\n",
        "print(f\"\\n✅ Updated manifest (durations filled where missing): {MANIFEST}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uvIpdb0km6Z",
        "outputId": "fb624ab6-754c-4c02-95ed-f03806d0b26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows needing duration compute: 0 / 8554\n",
            "Filtered invalid rows: 0 removed, 8554 remain\n",
            "\n",
            "=== Clips per language ===\n",
            "language\n",
            "english    6656\n",
            "hindi      1727\n",
            "punjabi     171\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Hours per language ===\n",
            "language\n",
            "english    9.61\n",
            "hindi      2.46\n",
            "punjabi    0.20\n",
            "Name: duration, dtype: float64\n",
            "\n",
            "=== Duration stats (seconds) ===\n",
            "          mean  median    p95\n",
            "language                     \n",
            "english   5.20    4.21  13.86\n",
            "hindi     5.13    5.08   7.72\n",
            "punjabi   4.25    4.10   5.78\n",
            "\n",
            "=== Totals ===\n",
            "Total clips: 8554\n",
            "Total hours: 12.27\n",
            "\n",
            "✅ Updated manifest (durations filled where missing): /content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Simple 10-min session mixer with quotas & light overlaps ===\n",
        "!pip -q install soundfile numpy pandas\n",
        "\n",
        "import os, json, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "\n",
        "MANIFEST = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "OUT_DIR  = \"/content/drive/MyDrive/problem_statement_6/sessions_smooth\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Mixer config ---\n",
        "SESSION_SEC        = 600            # target ~10 minutes\n",
        "GAP_RANGE_SEC      = (0.2, 0.8)     # gaps between turns\n",
        "OVERLAP_PROB       = 0.12           # ~12% turns overlap\n",
        "OVERLAP_SEC_RANGE  = (0.5, 1.5)\n",
        "SPEAKERS_PER_SESS  = (3, 4)\n",
        "SR_TARGET          = 16000          # assume files are at 16 kHz (CV already is; normalize EN earlier if needed)\n",
        "\n",
        "# Language quotas per session (fractions; will be normalized)\n",
        "LANG_QUOTAS = {\n",
        "    \"english\": 0.34,\n",
        "    \"hindi\":   0.33,\n",
        "    \"punjabi\": 0.33,  # oversample PA despite low hours\n",
        "}\n",
        "\n",
        "# Sessions to create\n",
        "N_SESSIONS = 20\n",
        "\n",
        "# --- Load & pre-bucket by language ---\n",
        "df = pd.read_csv(MANIFEST)\n",
        "df = df[df[\"language\"].isin([\"english\",\"hindi\",\"punjabi\"])]\n",
        "df = df[df[\"transcript\"].fillna(\"\").astype(str).str.strip().ne(\"\")]\n",
        "\n",
        "# Optional: trim extreme durations\n",
        "if \"duration\" in df.columns:\n",
        "    df = df[(df[\"duration\"] > 0.5) & (df[\"duration\"] < 30)]\n",
        "\n",
        "buckets = {lang: df[df[\"language\"]==lang].copy() for lang in [\"english\",\"hindi\",\"punjabi\"]}\n",
        "\n",
        "def load_wav(path, target_sr=SR_TARGET):\n",
        "    data, sr = sf.read(path, always_2d=False)\n",
        "    if data.ndim == 2:\n",
        "        data = data.mean(axis=1)\n",
        "    if sr != target_sr:\n",
        "        # Simple resample via linear interpolation to avoid heavy deps\n",
        "        x = np.arange(len(data))\n",
        "        new_len = int(len(data) * (target_sr / sr))\n",
        "        xi = np.linspace(0, len(data)-1, new_len)\n",
        "        data = np.interp(xi, x, data).astype(np.float32)\n",
        "        sr = target_sr\n",
        "    return data.astype(np.float32), sr\n",
        "\n",
        "def pick_speakers(rows, k):\n",
        "    # Use file path stem as provisional speaker; ensures multiple turns per file may map to same spk\n",
        "    # For simplicity, pick k *unique stems* across languages\n",
        "    stems = rows[\"audio_file\"].apply(lambda p: os.path.splitext(os.path.basename(p))[0])\n",
        "    uniq = rows.assign(stem=stems).drop_duplicates(subset=[\"stem\"])\n",
        "    return uniq.sample(min(k, len(uniq)), random_state=random.randint(0,1_000_000))[\"stem\"].tolist()\n",
        "\n",
        "def build_session(session_idx):\n",
        "    # Decide speakers per session\n",
        "    k = random.randint(*SPEAKERS_PER_SESS)\n",
        "\n",
        "    # Compute target seconds per language\n",
        "    total_weight = sum(LANG_QUOTAS.values())\n",
        "    quotas = {l: (v/total_weight) * SESSION_SEC for l,v in LANG_QUOTAS.items()}\n",
        "\n",
        "    # Candidate pool for each language\n",
        "    pools = {l: buckets[l].sample(frac=1.0, random_state=random.randint(0,1_000_000)).reset_index(drop=True) for l in buckets}\n",
        "    # Choose provisional speakers from the whole df for variety\n",
        "    spk_list = pick_speakers(df, k)\n",
        "    spk_map = {f\"spk{i+1:02d}\": s for i, s in enumerate(spk_list)}\n",
        "\n",
        "    timeline = []  # list of segments with audio arrays and metadata\n",
        "    cur_t = 0.0\n",
        "\n",
        "    # Greedy fill by language quotas\n",
        "    while cur_t < SESSION_SEC - 1.0:\n",
        "        # Choose language weighted by remaining quota\n",
        "        rem = {l: max(0.0, quotas[l]) for l in quotas}\n",
        "        lang = random.choices(list(rem.keys()), weights=list(rem.values()), k=1)[0]\n",
        "\n",
        "        # Draw a clip from that language\n",
        "        pool = pools[lang]\n",
        "        if pool.empty:\n",
        "            # fallback to any language with remaining quota\n",
        "            lang = random.choice([\"english\",\"hindi\",\"punjabi\"])\n",
        "            pool = pools[lang]\n",
        "            if pool.empty:\n",
        "                break\n",
        "\n",
        "        row = pool.iloc[0]\n",
        "        pools[lang] = pool.iloc[1:]  # pop\n",
        "\n",
        "        # Assign a session speaker randomly\n",
        "        sess_spk = random.choice(list(spk_map.keys()))\n",
        "        # Load audio\n",
        "        wav, sr = load_wav(row[\"audio_file\"], SR_TARGET)\n",
        "        dur = len(wav) / SR_TARGET\n",
        "\n",
        "        # Decide overlap or gap\n",
        "        if timeline and random.random() < OVERLAP_PROB:\n",
        "            ov = random.uniform(*OVERLAP_SEC_RANGE)\n",
        "            start_ts = max(0.0, cur_t - ov)\n",
        "        else:\n",
        "            gap = random.uniform(*GAP_RANGE_SEC)\n",
        "            start_ts = cur_t + gap\n",
        "\n",
        "        end_ts = start_ts + dur\n",
        "        cur_t = max(cur_t, end_ts)\n",
        "\n",
        "        timeline.append({\n",
        "            \"audio\": wav,\n",
        "            \"start_ts\": start_ts,\n",
        "            \"end_ts\": end_ts,\n",
        "            \"session_speaker\": sess_spk,\n",
        "            \"speaker_proxy\": spk_map[sess_spk],\n",
        "            \"language\": lang,\n",
        "            \"transcript\": str(row[\"transcript\"]),\n",
        "            \"source_file\": row[\"audio_file\"]\n",
        "        })\n",
        "\n",
        "        # Reduce language quota\n",
        "        quotas[lang] -= dur\n",
        "        if cur_t >= SESSION_SEC:\n",
        "            break\n",
        "\n",
        "    # Render mixed audio to buffer\n",
        "    if not timeline:\n",
        "        return None\n",
        "\n",
        "    # Determine final length\n",
        "    T = math.ceil(max(seg[\"end_ts\"] for seg in timeline) * SR_TARGET)\n",
        "    mix = np.zeros(T, dtype=np.float32)\n",
        "\n",
        "    for seg in timeline:\n",
        "        a = seg[\"audio\"]\n",
        "        s = int(round(seg[\"start_ts\"] * SR_TARGET))\n",
        "        e = s + len(a)\n",
        "        if e > len(mix):\n",
        "            mix = np.pad(mix, (0, e - len(mix)))\n",
        "        mix[s:e] += a  # naive sum; could clamp later to prevent clipping\n",
        "\n",
        "    # Light peak limiting to avoid clipping\n",
        "    peak = np.max(np.abs(mix)) if mix.size else 1.0\n",
        "    if peak > 1.0:\n",
        "        mix = mix / peak\n",
        "\n",
        "    # Write files\n",
        "    out_wav = os.path.join(OUT_DIR, f\"session_{session_idx:03d}.wav\")\n",
        "    sf.write(out_wav, mix, SR_TARGET)\n",
        "\n",
        "    # Segment manifest line-per-line\n",
        "    out_manifest = os.path.join(OUT_DIR, f\"session_{session_idx:03d}.jsonl\")\n",
        "    with open(out_manifest, \"w\", encoding=\"utf-8\") as f:\n",
        "        for seg in timeline:\n",
        "            rec = {\n",
        "                \"audio_file\": os.path.basename(out_wav),\n",
        "                \"start_ts\": round(seg[\"start_ts\"], 3),\n",
        "                \"end_ts\": round(seg[\"end_ts\"], 3),\n",
        "                \"speaker\": seg[\"session_speaker\"],      # for SD\n",
        "                \"language\": seg[\"language\"],             # for LID\n",
        "                \"asr_text\": seg[\"transcript\"],           # for ASR\n",
        "                \"translation_en\": \"\",                    # fill later via NMT if needed\n",
        "                \"source_path\": seg[\"source_file\"],\n",
        "                \"speaker_proxy\": seg[\"speaker_proxy\"]\n",
        "            }\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return out_wav, out_manifest, len(timeline)\n",
        "\n",
        "# Build sessions\n",
        "made = 0\n",
        "for i in range(N_SESSIONS):\n",
        "    res = build_session(i+1)\n",
        "    if res:\n",
        "        print(f\"✅ Session {i+1:03d}: wrote {res[0]} & {res[1]} with {res[2]} segments\")\n",
        "        made += 1\n",
        "    else:\n",
        "        print(f\"⚠️ Session {i+1:03d}: no content\")\n",
        "\n",
        "print(f\"\\nDone. Sessions created: {made}/{N_SESSIONS}\")\n",
        "print(\"Output dir:\", OUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaeudzWbmgms",
        "outputId": "ac03fd46-098e-46ce-ed8c-19b1a7ac7386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Session 001: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_001.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_001.jsonl with 113 segments\n",
            "✅ Session 002: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_002.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_002.jsonl with 113 segments\n",
            "✅ Session 003: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_003.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_003.jsonl with 113 segments\n",
            "✅ Session 004: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_004.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_004.jsonl with 124 segments\n",
            "✅ Session 005: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_005.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_005.jsonl with 112 segments\n",
            "✅ Session 006: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_006.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_006.jsonl with 125 segments\n",
            "✅ Session 007: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_007.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_007.jsonl with 115 segments\n",
            "✅ Session 008: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_008.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_008.jsonl with 116 segments\n",
            "✅ Session 009: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_009.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_009.jsonl with 112 segments\n",
            "✅ Session 010: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_010.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_010.jsonl with 120 segments\n",
            "✅ Session 011: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_011.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_011.jsonl with 102 segments\n",
            "✅ Session 012: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_012.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_012.jsonl with 106 segments\n",
            "✅ Session 013: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_013.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_013.jsonl with 117 segments\n",
            "✅ Session 014: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_014.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_014.jsonl with 121 segments\n",
            "✅ Session 015: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_015.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_015.jsonl with 117 segments\n",
            "✅ Session 016: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_016.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_016.jsonl with 103 segments\n",
            "✅ Session 017: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_017.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_017.jsonl with 116 segments\n",
            "✅ Session 018: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_018.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_018.jsonl with 113 segments\n",
            "✅ Session 019: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_019.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_019.jsonl with 113 segments\n",
            "✅ Session 020: wrote /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_020.wav & /content/drive/MyDrive/problem_statement_6/sessions_smooth/session_020.jsonl with 120 segments\n",
            "\n",
            "Done. Sessions created: 20/20\n",
            "Output dir: /content/drive/MyDrive/problem_statement_6/sessions_smooth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Simple 10-min session mixer with quotas & light overlaps ===\n",
        "!pip -q install soundfile numpy pandas\n",
        "\n",
        "import os, json, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "\n",
        "MANIFEST = \"/content/drive/MyDrive/problem_statement_6/training_data/foundational_manifest.csv\"\n",
        "OUT_DIR  = \"/content/drive/MyDrive/problem_statement_6/sessions_balanced\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Mixer config ---\n",
        "SESSION_SEC        = 600            # target ~10 minutes\n",
        "GAP_RANGE_SEC      = (0.2, 0.8)     # gaps between turns\n",
        "OVERLAP_PROB       = 0.12           # ~12% turns overlap\n",
        "OVERLAP_SEC_RANGE  = (0.5, 1.5)\n",
        "SPEAKERS_PER_SESS  = (3, 4)\n",
        "SR_TARGET          = 16000          # assume files are at 16 kHz (CV already is; normalize EN earlier if needed)\n",
        "\n",
        "# Language quotas per session (fractions; will be normalized)\n",
        "LANG_QUOTAS = {\n",
        "    \"english\": 0.60,\n",
        "    \"hindi\":   0.30,\n",
        "    \"punjabi\": 0.10,  # oversample PA despite low hours\n",
        "}\n",
        "\n",
        "# Sessions to create\n",
        "N_SESSIONS = 20\n",
        "\n",
        "# --- Load & pre-bucket by language ---\n",
        "df = pd.read_csv(MANIFEST)\n",
        "df = df[df[\"language\"].isin([\"english\",\"hindi\",\"punjabi\"])]\n",
        "df = df[df[\"transcript\"].fillna(\"\").astype(str).str.strip().ne(\"\")]\n",
        "\n",
        "# Optional: trim extreme durations\n",
        "if \"duration\" in df.columns:\n",
        "    df = df[(df[\"duration\"] > 0.5) & (df[\"duration\"] < 30)]\n",
        "\n",
        "buckets = {lang: df[df[\"language\"]==lang].copy() for lang in [\"english\",\"hindi\",\"punjabi\"]}\n",
        "\n",
        "def load_wav(path, target_sr=SR_TARGET):\n",
        "    data, sr = sf.read(path, always_2d=False)\n",
        "    if data.ndim == 2:\n",
        "        data = data.mean(axis=1)\n",
        "    if sr != target_sr:\n",
        "        # Simple resample via linear interpolation to avoid heavy deps\n",
        "        x = np.arange(len(data))\n",
        "        new_len = int(len(data) * (target_sr / sr))\n",
        "        xi = np.linspace(0, len(data)-1, new_len)\n",
        "        data = np.interp(xi, x, data).astype(np.float32)\n",
        "        sr = target_sr\n",
        "    return data.astype(np.float32), sr\n",
        "\n",
        "def pick_speakers(rows, k):\n",
        "    # Use file path stem as provisional speaker; ensures multiple turns per file may map to same spk\n",
        "    # For simplicity, pick k *unique stems* across languages\n",
        "    stems = rows[\"audio_file\"].apply(lambda p: os.path.splitext(os.path.basename(p))[0])\n",
        "    uniq = rows.assign(stem=stems).drop_duplicates(subset=[\"stem\"])\n",
        "    return uniq.sample(min(k, len(uniq)), random_state=random.randint(0,1_000_000))[\"stem\"].tolist()\n",
        "\n",
        "def build_session(session_idx):\n",
        "    # Decide speakers per session\n",
        "    k = random.randint(*SPEAKERS_PER_SESS)\n",
        "\n",
        "    # Compute target seconds per language\n",
        "    total_weight = sum(LANG_QUOTAS.values())\n",
        "    quotas = {l: (v/total_weight) * SESSION_SEC for l,v in LANG_QUOTAS.items()}\n",
        "\n",
        "    # Candidate pool for each language\n",
        "    pools = {l: buckets[l].sample(frac=1.0, random_state=random.randint(0,1_000_000)).reset_index(drop=True) for l in buckets}\n",
        "    # Choose provisional speakers from the whole df for variety\n",
        "    spk_list = pick_speakers(df, k)\n",
        "    spk_map = {f\"spk{i+1:02d}\": s for i, s in enumerate(spk_list)}\n",
        "\n",
        "    timeline = []  # list of segments with audio arrays and metadata\n",
        "    cur_t = 0.0\n",
        "\n",
        "    # Greedy fill by language quotas\n",
        "    while cur_t < SESSION_SEC - 1.0:\n",
        "        # Choose language weighted by remaining quota\n",
        "        rem = {l: max(0.0, quotas[l]) for l in quotas}\n",
        "        lang = random.choices(list(rem.keys()), weights=list(rem.values()), k=1)[0]\n",
        "\n",
        "        # Draw a clip from that language\n",
        "        pool = pools[lang]\n",
        "        if pool.empty:\n",
        "            # fallback to any language with remaining quota\n",
        "            lang = random.choice([\"english\",\"hindi\",\"punjabi\"])\n",
        "            pool = pools[lang]\n",
        "            if pool.empty:\n",
        "                break\n",
        "\n",
        "        row = pool.iloc[0]\n",
        "        pools[lang] = pool.iloc[1:]  # pop\n",
        "\n",
        "        # Assign a session speaker randomly\n",
        "        sess_spk = random.choice(list(spk_map.keys()))\n",
        "        # Load audio\n",
        "        wav, sr = load_wav(row[\"audio_file\"], SR_TARGET)\n",
        "        dur = len(wav) / SR_TARGET\n",
        "\n",
        "        # Decide overlap or gap\n",
        "        if timeline and random.random() < OVERLAP_PROB:\n",
        "            ov = random.uniform(*OVERLAP_SEC_RANGE)\n",
        "            start_ts = max(0.0, cur_t - ov)\n",
        "        else:\n",
        "            gap = random.uniform(*GAP_RANGE_SEC)\n",
        "            start_ts = cur_t + gap\n",
        "\n",
        "        end_ts = start_ts + dur\n",
        "        cur_t = max(cur_t, end_ts)\n",
        "\n",
        "        timeline.append({\n",
        "            \"audio\": wav,\n",
        "            \"start_ts\": start_ts,\n",
        "            \"end_ts\": end_ts,\n",
        "            \"session_speaker\": sess_spk,\n",
        "            \"speaker_proxy\": spk_map[sess_spk],\n",
        "            \"language\": lang,\n",
        "            \"transcript\": str(row[\"transcript\"]),\n",
        "            \"source_file\": row[\"audio_file\"]\n",
        "        })\n",
        "\n",
        "        # Reduce language quota\n",
        "        quotas[lang] -= dur\n",
        "        if cur_t >= SESSION_SEC:\n",
        "            break\n",
        "\n",
        "    # Render mixed audio to buffer\n",
        "    if not timeline:\n",
        "        return None\n",
        "\n",
        "    # Determine final length\n",
        "    T = math.ceil(max(seg[\"end_ts\"] for seg in timeline) * SR_TARGET)\n",
        "    mix = np.zeros(T, dtype=np.float32)\n",
        "\n",
        "    for seg in timeline:\n",
        "        a = seg[\"audio\"]\n",
        "        s = int(round(seg[\"start_ts\"] * SR_TARGET))\n",
        "        e = s + len(a)\n",
        "        if e > len(mix):\n",
        "            mix = np.pad(mix, (0, e - len(mix)))\n",
        "        mix[s:e] += a  # naive sum; could clamp later to prevent clipping\n",
        "\n",
        "    # Light peak limiting to avoid clipping\n",
        "    peak = np.max(np.abs(mix)) if mix.size else 1.0\n",
        "    if peak > 1.0:\n",
        "        mix = mix / peak\n",
        "\n",
        "    # Write files\n",
        "    out_wav = os.path.join(OUT_DIR, f\"session_{session_idx:03d}.wav\")\n",
        "    sf.write(out_wav, mix, SR_TARGET)\n",
        "\n",
        "    # Segment manifest line-per-line\n",
        "    out_manifest = os.path.join(OUT_DIR, f\"session_{session_idx:03d}.jsonl\")\n",
        "    with open(out_manifest, \"w\", encoding=\"utf-8\") as f:\n",
        "        for seg in timeline:\n",
        "            rec = {\n",
        "                \"audio_file\": os.path.basename(out_wav),\n",
        "                \"start_ts\": round(seg[\"start_ts\"], 3),\n",
        "                \"end_ts\": round(seg[\"end_ts\"], 3),\n",
        "                \"speaker\": seg[\"session_speaker\"],      # for SD\n",
        "                \"language\": seg[\"language\"],             # for LID\n",
        "                \"asr_text\": seg[\"transcript\"],           # for ASR\n",
        "                \"translation_en\": \"\",                    # fill later via NMT if needed\n",
        "                \"source_path\": seg[\"source_file\"],\n",
        "                \"speaker_proxy\": seg[\"speaker_proxy\"]\n",
        "            }\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return out_wav, out_manifest, len(timeline)\n",
        "\n",
        "# Build sessions\n",
        "made = 0\n",
        "for i in range(N_SESSIONS):\n",
        "    res = build_session(i+1)\n",
        "    if res:\n",
        "        print(f\"✅ Session {i+1:03d}: wrote {res[0]} & {res[1]} with {res[2]} segments\")\n",
        "        made += 1\n",
        "    else:\n",
        "        print(f\"⚠️ Session {i+1:03d}: no content\")\n",
        "\n",
        "print(f\"\\nDone. Sessions created: {made}/{N_SESSIONS}\")\n",
        "print(\"Output dir:\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH2E0ElzprHW",
        "outputId": "d5a4e8bf-4f1f-4fad-b2c6-b2c38556324c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Session 001: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_001.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_001.jsonl with 118 segments\n",
            "✅ Session 002: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_002.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_002.jsonl with 101 segments\n",
            "✅ Session 003: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_003.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_003.jsonl with 107 segments\n",
            "✅ Session 004: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_004.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_004.jsonl with 109 segments\n",
            "✅ Session 005: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_005.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_005.jsonl with 103 segments\n",
            "✅ Session 006: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_006.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_006.jsonl with 106 segments\n",
            "✅ Session 007: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_007.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_007.jsonl with 109 segments\n",
            "✅ Session 008: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_008.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_008.jsonl with 115 segments\n",
            "✅ Session 009: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_009.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_009.jsonl with 102 segments\n",
            "✅ Session 010: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_010.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_010.jsonl with 110 segments\n",
            "✅ Session 011: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_011.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_011.jsonl with 108 segments\n",
            "✅ Session 012: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_012.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_012.jsonl with 108 segments\n",
            "✅ Session 013: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_013.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_013.jsonl with 108 segments\n",
            "✅ Session 014: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_014.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_014.jsonl with 99 segments\n",
            "✅ Session 015: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_015.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_015.jsonl with 102 segments\n",
            "✅ Session 016: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_016.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_016.jsonl with 110 segments\n",
            "✅ Session 017: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_017.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_017.jsonl with 110 segments\n",
            "✅ Session 018: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_018.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_018.jsonl with 98 segments\n",
            "✅ Session 019: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_019.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_019.jsonl with 104 segments\n",
            "✅ Session 020: wrote /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_020.wav & /content/drive/MyDrive/problem_statement_6/sessions_balanced/session_020.jsonl with 105 segments\n",
            "\n",
            "Done. Sessions created: 20/20\n",
            "Output dir: /content/drive/MyDrive/problem_statement_6/sessions_balanced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Session QA: format, timing, language mix, overlap %, speaker consistency ---\n",
        "!pip -q install soundfile\n",
        "import os, json, math, collections\n",
        "import soundfile as sf\n",
        "\n",
        "SESS_DIRS = [\n",
        "    \"/content/drive/MyDrive/problem_statement_6/sessions_smooth\",\n",
        "    \"/content/drive/MyDrive/problem_statement_6/sessions_balanced\",\n",
        "]\n",
        "\n",
        "def merge_intervals(ivls):\n",
        "    # ivls: list of (start, end)\n",
        "    if not ivls: return 0.0\n",
        "    ivls = sorted(ivls, key=lambda x: x[0])\n",
        "    merged = []\n",
        "    cs, ce = ivls[0]\n",
        "    for s,e in ivls[1:]:\n",
        "        if s <= ce:\n",
        "            ce = max(ce, e)\n",
        "        else:\n",
        "            merged.append((cs, ce))\n",
        "            cs, ce = s, e\n",
        "    merged.append((cs, ce))\n",
        "    return sum(e - s for s, e in merged)\n",
        "\n",
        "def qa_folder(folder):\n",
        "    print(f\"\\n====== QA: {folder} ======\")\n",
        "    sessions = sorted([f for f in os.listdir(folder) if f.endswith(\".jsonl\")])\n",
        "    if not sessions:\n",
        "        print(\"No .jsonl found.\")\n",
        "        return\n",
        "    summary = []\n",
        "    bad_format_examples = []\n",
        "    spk_mix_warnings = 0\n",
        "\n",
        "    for jfile in sessions:\n",
        "        base = jfile[:-6]\n",
        "        wav = os.path.join(folder, base + \".wav\")\n",
        "        jpath = os.path.join(folder, jfile)\n",
        "\n",
        "        # Audio info\n",
        "        if not os.path.exists(wav):\n",
        "            print(f\"  ! Missing WAV for {jfile}\")\n",
        "            continue\n",
        "        try:\n",
        "            info = sf.info(wav)\n",
        "        except Exception as e:\n",
        "            print(f\"  ! Unreadable WAV {wav}: {e}\")\n",
        "            continue\n",
        "\n",
        "        fmt_ok = (info.samplerate == 16000 and info.channels == 1 and \"PCM_16\" in info.subtype)\n",
        "        if not fmt_ok and len(bad_format_examples) < 3:\n",
        "            bad_format_examples.append((os.path.basename(wav), info.samplerate, info.channels, info.subtype))\n",
        "\n",
        "        # Manifest parse\n",
        "        segs, langs, spk2proxies = [], collections.Counter(), {}\n",
        "        with open(jpath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if not line.strip(): continue\n",
        "                rec = json.loads(line)\n",
        "                s, e = float(rec[\"start_ts\"]), float(rec[\"end_ts\"])\n",
        "                if e <= s:  # skip bogus\n",
        "                    continue\n",
        "                segs.append((s, e, rec.get(\"language\",\"\").lower(), rec.get(\"speaker\",\"\"), rec.get(\"speaker_proxy\",\"\")))\n",
        "                langs[rec.get(\"language\",\"\").lower()] += (e - s)\n",
        "                spk = rec.get(\"speaker\",\"\")\n",
        "                spk2proxies.setdefault(spk, set()).add(rec.get(\"speaker_proxy\",\"\"))\n",
        "\n",
        "        if not segs:\n",
        "            print(f\"  ! No valid segments in {jfile}\")\n",
        "            continue\n",
        "\n",
        "        # Duration checks\n",
        "        seg_total = sum(e - s for s, e, *_ in segs)\n",
        "        union_dur = merge_intervals([(s,e) for s, e, *_ in segs])\n",
        "        overlap_dur = max(0.0, seg_total - union_dur)\n",
        "        overlap_pct = 100.0 * overlap_dur / union_dur if union_dur > 0 else 0.0\n",
        "\n",
        "        # Speaker consistency: any spkXX mapped to multiple proxies?\n",
        "        mixed = {spk: len(proxies) for spk, proxies in spk2proxies.items() if len(proxies) > 1}\n",
        "        if mixed:\n",
        "            spk_mix_warnings += 1\n",
        "\n",
        "        # Compare union duration to actual audio length\n",
        "        audio_len = info.frames / info.samplerate if info.samplerate else 0.0\n",
        "        dur_gap_sec = abs(audio_len - union_dur)\n",
        "\n",
        "        summary.append({\n",
        "            \"session\": base,\n",
        "            \"segments\": len(segs),\n",
        "            \"audio_sec\": round(audio_len, 1),\n",
        "            \"union_sec\": round(union_dur, 1),\n",
        "            \"dur_gap_s\": round(dur_gap_sec, 2),\n",
        "            \"overlap_pct\": round(overlap_pct, 1),\n",
        "            \"fmt_ok\": fmt_ok,\n",
        "            \"lang_share\": {k: round(100*v/union_dur,1) for k,v in langs.items() if union_dur>0},\n",
        "            \"spk_mixed\": bool(mixed)\n",
        "        })\n",
        "\n",
        "    # Print a compact report\n",
        "    ok = sum(1 for r in summary if r[\"fmt_ok\"])\n",
        "    print(f\"Sessions scanned: {len(summary)}\")\n",
        "    print(f\" PCM_16/16k/mono OK: {ok}/{len(summary)}\")\n",
        "    if bad_format_examples:\n",
        "        print(\" Example non-PCM16 files (name, sr, ch, subtype):\", bad_format_examples)\n",
        "\n",
        "    avg_overlap = round(sum(r[\"overlap_pct\"] for r in summary)/max(1,len(summary)), 1)\n",
        "    avg_segs = round(sum(r[\"segments\"] for r in summary)/max(1,len(summary)), 1)\n",
        "    avg_dur_gap = round(sum(r[\"dur_gap_s\"] for r in summary)/max(1,len(summary)), 2)\n",
        "    print(f\" Avg segments/session: {avg_segs}\")\n",
        "    print(f\" Avg overlap %: {avg_overlap}\")\n",
        "    print(f\" Avg |audio_len - union_len| (s): {avg_dur_gap}\")\n",
        "    print(f\" Sessions with speaker-mix warning (spkXX→multiple proxies): {spk_mix_warnings}/{len(summary)}\")\n",
        "\n",
        "    # Show first 5 sessions with key stats\n",
        "    print(\"\\nSample sessions:\")\n",
        "    for r in summary[:5]:\n",
        "        print(f\"  {r['session']}: segs={r['segments']}, dur={r['audio_sec']}s, overlap={r['overlap_pct']}%, fmt_ok={r['fmt_ok']}, spk_mixed={r['spk_mixed']}, langs={r['lang_share']}\")\n",
        "\n",
        "for d in SESS_DIRS:\n",
        "    qa_folder(d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTmx2qyMuUl0",
        "outputId": "2a3e30c0-9c4c-426c-b127-9827e211b8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== QA: /content/drive/MyDrive/problem_statement_6/sessions_smooth ======\n",
            "Sessions scanned: 20\n",
            " PCM_16/16k/mono OK: 20/20\n",
            " Avg segments/session: 114.5\n",
            " Avg overlap %: 2.3\n",
            " Avg |audio_len - union_len| (s): 50.68\n",
            " Sessions with speaker-mix warning (spkXX→multiple proxies): 0/20\n",
            "\n",
            "Sample sessions:\n",
            "  session_001: segs=113, dur=603.4s, overlap=2.0%, fmt_ok=True, spk_mixed=False, langs={'english': 36.4, 'hindi': 32.0, 'punjabi': 33.6}\n",
            "  session_002: segs=113, dur=601.2s, overlap=1.8%, fmt_ok=True, spk_mixed=False, langs={'punjabi': 31.8, 'english': 36.2, 'hindi': 33.8}\n",
            "  session_003: segs=113, dur=599.5s, overlap=2.4%, fmt_ok=True, spk_mixed=False, langs={'english': 35.5, 'punjabi': 32.2, 'hindi': 34.7}\n",
            "  session_004: segs=124, dur=603.1s, overlap=3.6%, fmt_ok=True, spk_mixed=False, langs={'punjabi': 34.2, 'english': 35.0, 'hindi': 34.4}\n",
            "  session_005: segs=112, dur=606.6s, overlap=2.4%, fmt_ok=True, spk_mixed=False, langs={'punjabi': 32.2, 'hindi': 35.5, 'english': 34.6}\n",
            "\n",
            "====== QA: /content/drive/MyDrive/problem_statement_6/sessions_balanced ======\n",
            "Sessions scanned: 20\n",
            " PCM_16/16k/mono OK: 20/20\n",
            " Avg segments/session: 106.6\n",
            " Avg overlap %: 2.1\n",
            " Avg |audio_len - union_len| (s): 46.91\n",
            " Sessions with speaker-mix warning (spkXX→multiple proxies): 0/20\n",
            "\n",
            "Sample sessions:\n",
            "  session_001: segs=118, dur=600.0s, overlap=1.7%, fmt_ok=True, spk_mixed=False, langs={'hindi': 30.6, 'english': 61.2, 'punjabi': 9.9}\n",
            "  session_002: segs=101, dur=601.6s, overlap=1.6%, fmt_ok=True, spk_mixed=False, langs={'english': 60.3, 'hindi': 31.6, 'punjabi': 9.7}\n",
            "  session_003: segs=107, dur=605.2s, overlap=2.7%, fmt_ok=True, spk_mixed=False, langs={'hindi': 30.1, 'english': 62.4, 'punjabi': 10.1}\n",
            "  session_004: segs=109, dur=603.2s, overlap=1.0%, fmt_ok=True, spk_mixed=False, langs={'hindi': 29.4, 'english': 61.8, 'punjabi': 9.8}\n",
            "  session_005: segs=103, dur=604.6s, overlap=1.9%, fmt_ok=True, spk_mixed=False, langs={'english': 61.6, 'hindi': 30.0, 'punjabi': 10.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameterized 10-min session generator\n",
        "!pip -q install soundfile numpy pandas\n",
        "\n",
        "import os, json, math, random, hashlib, re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "\n",
        "\n",
        "# --------- Small DSP helpers (no heavy deps) ---------\n",
        "\n",
        "def linear_resample(x: np.ndarray, sr_in: int, sr_out: int) -> np.ndarray:\n",
        "    if sr_in == sr_out or x.size == 0:\n",
        "        return x.astype(np.float32)\n",
        "    N = x.shape[-1]\n",
        "    new_N = max(1, int(round(N * sr_out / sr_in)))\n",
        "    xp = np.linspace(0, N - 1, num=N, dtype=np.float64)\n",
        "    xq = np.linspace(0, N - 1, num=new_N, dtype=np.float64)\n",
        "    y = np.interp(xq, xp, x.astype(np.float32)).astype(np.float32)\n",
        "    return y\n",
        "\n",
        "def to_mono(x: np.ndarray) -> np.ndarray:\n",
        "    if x.ndim == 2:\n",
        "        return x.mean(axis=1).astype(np.float32)\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "def add_awgn(x: np.ndarray, snr_db: float) -> np.ndarray:\n",
        "    # Adds white noise at target SNR (approximate)\n",
        "    if snr_db is None:\n",
        "        return x\n",
        "    p_signal = np.mean(x**2) + 1e-9\n",
        "    p_noise = p_signal / (10 ** (snr_db / 10.0))\n",
        "    noise = np.random.normal(0.0, np.sqrt(p_noise), size=x.shape).astype(np.float32)\n",
        "    y = x + noise\n",
        "    # light peak guard\n",
        "    peak = np.max(np.abs(y)) + 1e-9\n",
        "    if peak > 1.0:\n",
        "        y = y / peak\n",
        "    return y\n",
        "\n",
        "def bandlimit_8k_then_back_16k(x: np.ndarray, sr: int) -> np.ndarray:\n",
        "    # crude telephone channel: downsample to 8k, then back to 16k\n",
        "    if sr != 16000:\n",
        "        x = linear_resample(x, sr, 16000)\n",
        "        sr = 16000\n",
        "    y8 = linear_resample(x, 16000, 8000)\n",
        "    y16 = linear_resample(y8, 8000, 16000)\n",
        "    return y16\n",
        "\n",
        "# --------- Config ---------\n",
        "\n",
        "@dataclass\n",
        "class GenConfig:\n",
        "    # Core\n",
        "    session_seconds: int = 600\n",
        "    sr_target: int = 16000\n",
        "    speakers_per_session: Tuple[int, int] = (3, 4)\n",
        "\n",
        "    # Language mix (weights; will be normalized)\n",
        "    lang_weights: Dict[str, float] = None  # e.g., {\"english\":0.6,\"hindi\":0.3,\"punjabi\":0.1}\n",
        "\n",
        "    # Timing / structure\n",
        "    gap_sec_range: Tuple[float, float] = (0.2, 0.8)\n",
        "    overlap_prob: float = 0.12\n",
        "    overlap_sec_range: Tuple[float, float] = (0.5, 1.5)\n",
        "\n",
        "    # Clip duration constraints\n",
        "    min_clip_sec: float = 0.5\n",
        "    max_clip_sec: float = 30.0\n",
        "\n",
        "    # Speaker / turn shaping\n",
        "    min_turns_per_speaker: int = 8\n",
        "    max_turns_per_speaker: int = 99999\n",
        "    bind_key_by_lang: Dict[str, str] = None  # e.g., {\"hindi\":\"client_id\",\"punjabi\":\"client_id\",\"english\":\"speaker_proxy\"}\n",
        "\n",
        "    # Code-switching control\n",
        "    force_code_switch: bool = False         # if True, encourages alternating languages per speaker where possible\n",
        "    code_switch_prob: float = 0.5           # probability a speaker switches language for next turn (if available)\n",
        "\n",
        "    # Augmentation\n",
        "    snr_db_range: Optional[Tuple[float, float]] = None  # e.g., (15, 30) for mild AWGN\n",
        "    apply_tel_bandlimit_prob: float = 0.0               # e.g., 0.2 to simulate phone channel sometimes\n",
        "    global_gain_db: float = 0.0                         # e.g., -3.0 to reduce overall loudness\n",
        "\n",
        "    # Output\n",
        "    out_dir: str = \"\"\n",
        "    scenario_name: str = \"sessions_custom\"\n",
        "    seed: int = 1337\n",
        "\n",
        "\n",
        "class SessionGenerator:\n",
        "    def __init__(self, manifest_csv: str, config: GenConfig):\n",
        "        self.manifest_csv = manifest_csv\n",
        "        self.cfg = config\n",
        "        self.rng = random.Random(self.cfg.seed)\n",
        "        np.random.seed(self.cfg.seed)\n",
        "\n",
        "        assert os.path.exists(self.manifest_csv), f\"Manifest not found: {self.manifest_csv}\"\n",
        "        self.df = pd.read_csv(self.manifest_csv)\n",
        "        req_cols = {\"audio_file\",\"transcript\",\"language\"}\n",
        "        assert req_cols.issubset(self.df.columns), f\"Manifest must have columns: {req_cols}\"\n",
        "\n",
        "        # Clean & filter\n",
        "        self.df[\"language\"] = self.df[\"language\"].astype(str).str.strip().str.lower()\n",
        "        self.df = self.df[self.df[\"transcript\"].fillna(\"\").astype(str).str.strip().ne(\"\")]\n",
        "        if \"duration\" in self.df.columns:\n",
        "            self.df = self.df[(self.df[\"duration\"] > self.cfg.min_clip_sec) & (self.df[\"duration\"] < self.cfg.max_clip_sec)]\n",
        "\n",
        "        # Bind columns (speaker identity per language)\n",
        "        self.bind_key_by_lang = self.cfg.bind_key_by_lang or {}\n",
        "        # Create a robust fallback speaker proxy:\n",
        "        def default_proxy(row):\n",
        "            # stem as last resort\n",
        "            stem = os.path.splitext(os.path.basename(str(row[\"audio_file\"])))[0]\n",
        "            return stem\n",
        "        # Build a unified column \"spk_proxy\"\n",
        "        proxies = []\n",
        "        for _, r in self.df.iterrows():\n",
        "            lang = r[\"language\"]\n",
        "            key = self.bind_key_by_lang.get(lang, None)\n",
        "            if key and key in self.df.columns:\n",
        "                proxies.append(str(r[key]) if pd.notna(r[key]) else default_proxy(r))\n",
        "            else:\n",
        "                proxies.append(default_proxy(r))\n",
        "        self.df[\"spk_proxy\"] = proxies\n",
        "\n",
        "        # Pre-buckets by language; also index by speaker proxy per language\n",
        "        self.langs = sorted(self.df[\"language\"].unique())\n",
        "        self.lang_buckets = {l: self.df[self.df[\"language\"]==l].copy() for l in self.langs}\n",
        "        self.lang_spk_to_rows = {l: {} for l in self.langs}\n",
        "        for l in self.langs:\n",
        "            g = self.lang_buckets[l].groupby(\"spk_proxy\")\n",
        "            self.lang_spk_to_rows[l] = {spk: grp.sample(frac=1.0, random_state=self.rng.randint(0, 1_000_000)).reset_index(drop=True)\n",
        "                                        for spk, grp in g}\n",
        "\n",
        "        # Normalize language weights\n",
        "        if self.cfg.lang_weights is None:\n",
        "            # default: equal weights across present languages\n",
        "            self.cfg.lang_weights = {l: 1.0 for l in self.langs}\n",
        "        total_w = sum(max(0.0, w) for w in self.cfg.lang_weights.values())\n",
        "        self.lang_targets = {l: (self.cfg.lang_weights.get(l, 0.0) / total_w) * self.cfg.session_seconds for l in self.langs}\n",
        "\n",
        "        # Output dir\n",
        "        self.out_root = self.cfg.out_dir or os.path.join(os.path.dirname(self.manifest_csv), self.cfg.scenario_name)\n",
        "        os.makedirs(self.out_root, exist_ok=True)\n",
        "\n",
        "    # --------------- Audio IO & Aug ---------------\n",
        "\n",
        "    def load_mono_16k(self, path: str) -> np.ndarray:\n",
        "        x, sr = sf.read(path, always_2d=False)\n",
        "        x = to_mono(np.asarray(x))\n",
        "        if sr != self.cfg.sr_target:\n",
        "            x = linear_resample(x, sr, self.cfg.sr_target)\n",
        "        # Augment: telephone bandlimit?\n",
        "        if self.rng.random() < self.cfg.apply_tel_bandlimit_prob:\n",
        "            x = bandlimit_8k_then_back_16k(x, self.cfg.sr_target)\n",
        "        # Augment: AWGN?\n",
        "        if self.cfg.snr_db_range:\n",
        "            snr = self.rng.uniform(*self.cfg.snr_db_range)\n",
        "            x = add_awgn(x, snr)\n",
        "        # Gain\n",
        "        if self.cfg.global_gain_db != 0.0:\n",
        "            g = 10 ** (self.cfg.global_gain_db / 20.0)\n",
        "            x = x * g\n",
        "        # Soft peak guard\n",
        "        peak = np.max(np.abs(x)) + 1e-9\n",
        "        if peak > 1.0:\n",
        "            x = x / peak\n",
        "        return x.astype(np.float32)\n",
        "\n",
        "    # --------------- Speaker roster & language plan ---------------\n",
        "\n",
        "    def pick_session_speakers(self, k: int) -> Dict[str, Tuple[str, str]]:\n",
        "        \"\"\"\n",
        "        Returns mapping: spk01 -> (lang, spk_proxy), each tied to a *single* real proxy for that lang.\n",
        "        If code-switching is on, we only bind an initial language; speakers may switch later.\n",
        "        \"\"\"\n",
        "        # Start from languages with more target seconds first (to ensure coverage)\n",
        "        langs_sorted = sorted(self.langs, key=lambda l: self.lang_targets.get(l, 0.0), reverse=True)\n",
        "        roster = {}\n",
        "        tried = set()\n",
        "        for i in range(k):\n",
        "            # choose a language weighted by target seconds\n",
        "            weights = [max(1e-6, self.lang_targets.get(l, 0.0)) for l in langs_sorted]\n",
        "            lang = self.rng.choices(langs_sorted, weights=weights, k=1)[0]\n",
        "            # pick a speaker proxy for that language with enough remaining rows\n",
        "            spk_pool = list(self.lang_spk_to_rows[lang].keys())\n",
        "            self.rng.shuffle(spk_pool)\n",
        "            chosen = None\n",
        "            for spk in spk_pool:\n",
        "                if (lang, spk) in tried:  # avoid repeats in the same session\n",
        "                    continue\n",
        "                if len(self.lang_spk_to_rows[lang][spk]) >= self.cfg.min_turns_per_speaker:\n",
        "                    chosen = spk\n",
        "                    break\n",
        "            if not chosen:\n",
        "                # fallback to any speaker with at least 1 row\n",
        "                for spk in spk_pool:\n",
        "                    if (lang, spk) not in tried and len(self.lang_spk_to_rows[lang][spk]) > 0:\n",
        "                        chosen = spk\n",
        "                        break\n",
        "            if chosen is None:\n",
        "                # fallback to any language\n",
        "                for L in self.langs:\n",
        "                    spk_pool = list(self.lang_spk_to_rows[L].keys())\n",
        "                    if spk_pool:\n",
        "                        chosen = spk_pool[0]\n",
        "                        lang = L\n",
        "                        break\n",
        "            roster[f\"spk{i+1:02d}\"] = (lang, chosen)\n",
        "            tried.add((lang, chosen))\n",
        "        return roster\n",
        "\n",
        "    def pick_clip_for(self, lang: str, spk_proxy: str) -> Optional[pd.Series]:\n",
        "        rows = self.lang_spk_to_rows.get(lang, {}).get(spk_proxy, None)\n",
        "        if rows is None or len(rows) == 0:\n",
        "            return None\n",
        "        row = rows.iloc[0]\n",
        "        # pop\n",
        "        self.lang_spk_to_rows[lang][spk_proxy] = rows.iloc[1:]\n",
        "        return row\n",
        "\n",
        "    # --------------- Build one session ---------------\n",
        "\n",
        "    def build_session(self, session_idx: int) -> Optional[Tuple[str, str, int]]:\n",
        "        k = self.rng.randint(*self.cfg.speakers_per_session)\n",
        "        roster = self.pick_session_speakers(k)  # spkXX -> (lang, spk_proxy)\n",
        "\n",
        "        # Make a mutable language target seconds copy\n",
        "        lang_remain = dict(self.lang_targets)\n",
        "        # Create per-speaker language state (for code-switching)\n",
        "        spk_lang_state = {spk: roster[spk][0] for spk in roster}\n",
        "\n",
        "        timeline = []\n",
        "        cur_t = 0.0\n",
        "\n",
        "        while cur_t < self.cfg.session_seconds - 1.0:\n",
        "            # Choose next speaker to keep min/max turns roughly satisfied\n",
        "            spk_order = list(roster.keys())\n",
        "            self.rng.shuffle(spk_order)\n",
        "            chosen_spk = spk_order[0]\n",
        "            # Decide language for this turn\n",
        "            lang = spk_lang_state[chosen_spk]\n",
        "            if self.cfg.force_code_switch and self.rng.random() < self.cfg.code_switch_prob:\n",
        "                # attempt to switch language for this speaker (choose language with remaining quota)\n",
        "                lang_cands = sorted(self.langs, key=lambda l: lang_remain.get(l, 0.0), reverse=True)\n",
        "                for L in lang_cands:\n",
        "                    if L != lang:\n",
        "                        lang = L\n",
        "                        break\n",
        "                spk_lang_state[chosen_spk] = lang\n",
        "\n",
        "            # Pick a clip for (lang, speaker_proxy); if empty, relax to any speaker in lang; else any lang\n",
        "            _, spk_proxy = roster[chosen_spk]\n",
        "            row = self.pick_clip_for(lang, spk_proxy)\n",
        "            if row is None:\n",
        "                # try any speaker in this language\n",
        "                for other_spk in self.lang_spk_to_rows.get(lang, {}):\n",
        "                    row = self.pick_clip_for(lang, other_spk)\n",
        "                    if row is not None:\n",
        "                        # rebind chosen speaker to this proxy for session consistency\n",
        "                        roster[chosen_spk] = (lang, other_spk)\n",
        "                        break\n",
        "            if row is None:\n",
        "                # try any language weighted by remaining seconds\n",
        "                avail_langs = [L for L in self.langs if any(len(v) > 0 for v in self.lang_spk_to_rows[L].values())]\n",
        "                if not avail_langs:\n",
        "                    break\n",
        "                lang = self.rng.choices(avail_langs, weights=[max(1e-6, lang_remain.get(L, 0.0)) for L in avail_langs], k=1)[0]\n",
        "                # choose any speaker in that language\n",
        "                for other_spk in self.lang_spk_to_rows.get(lang, {}):\n",
        "                    row = self.pick_clip_for(lang, other_spk)\n",
        "                    if row is not None:\n",
        "                        roster[chosen_spk] = (lang, other_spk)\n",
        "                        spk_lang_state[chosen_spk] = lang\n",
        "                        break\n",
        "            if row is None:\n",
        "                break  # nothing left\n",
        "\n",
        "            # Load audio\n",
        "            wav = self.load_mono_16k(row[\"audio_file\"])\n",
        "            dur = len(wav) / self.cfg.sr_target\n",
        "\n",
        "            # Decide overlap/gap\n",
        "            if timeline and self.rng.random() < self.cfg.overlap_prob:\n",
        "                ov = self.rng.uniform(*self.cfg.overlap_sec_range)\n",
        "                start_ts = max(0.0, cur_t - ov)\n",
        "            else:\n",
        "                gap = self.rng.uniform(*self.cfg.gap_sec_range)\n",
        "                start_ts = cur_t + gap\n",
        "\n",
        "            end_ts = start_ts + dur\n",
        "            cur_t = max(cur_t, end_ts)\n",
        "\n",
        "            timeline.append({\n",
        "                \"audio\": wav,\n",
        "                \"start_ts\": start_ts,\n",
        "                \"end_ts\": end_ts,\n",
        "                \"session_speaker\": chosen_spk,\n",
        "                \"speaker_proxy\": roster[chosen_spk][1],\n",
        "                \"language\": lang,\n",
        "                \"transcript\": str(row[\"transcript\"]),\n",
        "                \"source_file\": row[\"audio_file\"]\n",
        "            })\n",
        "\n",
        "            # reduce remaining language target\n",
        "            lang_remain[lang] = max(0.0, lang_remain.get(lang, 0.0) - dur)\n",
        "            if cur_t >= self.cfg.session_seconds:\n",
        "                break\n",
        "\n",
        "        if not timeline:\n",
        "            return None\n",
        "\n",
        "        # Mix\n",
        "        T = int(math.ceil(max(seg[\"end_ts\"] for seg in timeline) * self.cfg.sr_target))\n",
        "        mix = np.zeros(T, dtype=np.float32)\n",
        "        for seg in timeline:\n",
        "            a = seg[\"audio\"]\n",
        "            s = int(round(seg[\"start_ts\"] * self.cfg.sr_target))\n",
        "            e = s + len(a)\n",
        "            if e > len(mix):\n",
        "                mix = np.pad(mix, (0, e - len(mix)))\n",
        "            mix[s:e] += a\n",
        "\n",
        "        # Peak limit\n",
        "        peak = np.max(np.abs(mix)) + 1e-9\n",
        "        if peak > 1.0:\n",
        "            mix = mix / peak\n",
        "\n",
        "        # Write outputs (PCM16)\n",
        "        out_wav = os.path.join(self.out_root, f\"session_{session_idx:03d}.wav\")\n",
        "        sf.write(out_wav, (mix * 32767.0).astype(np.int16), self.cfg.sr_target, subtype='PCM_16')\n",
        "\n",
        "        out_manifest = os.path.join(self.out_root, f\"session_{session_idx:03d}.jsonl\")\n",
        "        with open(out_manifest, \"w\", encoding=\"utf-8\") as f:\n",
        "            for seg in timeline:\n",
        "                rec = {\n",
        "                    \"audio_file\": os.path.basename(out_wav),\n",
        "                    \"start_ts\": round(seg[\"start_ts\"], 3),\n",
        "                    \"end_ts\": round(seg[\"end_ts\"], 3),\n",
        "                    \"speaker\": seg[\"session_speaker\"],\n",
        "                    \"language\": seg[\"language\"],\n",
        "                    \"asr_text\": seg[\"transcript\"],\n",
        "                    \"translation_en\": \"\",\n",
        "                    \"source_path\": seg[\"source_file\"],\n",
        "                    \"speaker_proxy\": seg[\"speaker_proxy\"]\n",
        "                }\n",
        "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        # Write a roster sidecar for audit\n",
        "        roster_map = {spk: {\"initial_lang\": lang, \"speaker_proxy\": proxy} for spk,(lang,proxy) in roster.items()}\n",
        "        with open(os.path.join(self.out_root, f\"session_{session_idx:03d}_roster.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(roster_map, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return out_wav, out_manifest, len(timeline)\n",
        "\n",
        "    # --------------- Batch API ---------------\n",
        "\n",
        "    def generate(self, n_sessions: int) -> None:\n",
        "        made = 0\n",
        "        for i in range(1, n_sessions + 1):\n",
        "            res = self.build_session(i)\n",
        "            if res:\n",
        "                print(f\"✅ {self.cfg.scenario_name} {i:03d}: wrote {res[0]} & {res[1]} with {res[2]} segments\")\n",
        "                made += 1\n",
        "            else:\n",
        "                print(f\"⚠️ {self.cfg.scenario_name} {i:03d}: no content\")\n",
        "        print(f\"\\nDone. Sessions created: {made}/{n_sessions} → {self.out_root}\")\n",
        "\n",
        "\n",
        "# ------------------- EXAMPLES (create scenarios) -------------------\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/problem_statement_6\"\n",
        "MANIFEST = f\"{BASE}/training_data/foundational_manifest.csv\"\n",
        "\n",
        "# 1) Balanced 33/33/33 with low overlap (baseline)\n",
        "cfg_bal = GenConfig(\n",
        "    session_seconds=600,\n",
        "    lang_weights={\"english\":1,\"hindi\":1,\"punjabi\":1},\n",
        "    gap_sec_range=(0.2, 0.8),\n",
        "    overlap_prob=0.08,\n",
        "    overlap_sec_range=(0.4, 1.2),\n",
        "    speakers_per_session=(3,4),\n",
        "    min_clip_sec=0.5, max_clip_sec=20.0,\n",
        "    bind_key_by_lang={\"hindi\":\"client_id\",\"punjabi\":\"client_id\",\"english\":\"spk_proxy\"},\n",
        "    force_code_switch=False,\n",
        "    snr_db_range=None,\n",
        "    apply_tel_bandlimit_prob=0.0,\n",
        "    global_gain_db=0.0,\n",
        "    out_dir=f\"{BASE}/sessions_balanced\",\n",
        "    scenario_name=\"sessions_balanced\",\n",
        "    seed=123\n",
        ")\n",
        "\n",
        "# 2) Smoothed 60/30/10 with modest overlap\n",
        "cfg_smooth = GenConfig(\n",
        "    session_seconds=600,\n",
        "    lang_weights={\"english\":0.60,\"hindi\":0.30,\"punjabi\":0.10},\n",
        "    gap_sec_range=(0.2, 0.8),\n",
        "    overlap_prob=0.12,\n",
        "    overlap_sec_range=(0.5, 1.5),\n",
        "    speakers_per_session=(3,4),\n",
        "    min_clip_sec=0.5, max_clip_sec=20.0,\n",
        "    bind_key_by_lang={\"hindi\":\"client_id\",\"punjabi\":\"client_id\",\"english\":\"spk_proxy\"},\n",
        "    force_code_switch=False,\n",
        "    snr_db_range=None,\n",
        "    apply_tel_bandlimit_prob=0.0,\n",
        "    global_gain_db=0.0,\n",
        "    out_dir=f\"{BASE}/sessions_smooth\",\n",
        "    scenario_name=\"sessions_smooth\",\n",
        "    seed=124\n",
        ")\n",
        "\n",
        "# 3) Proportional ~78/20/2 (raw prior)\n",
        "cfg_prop = GenConfig(\n",
        "    session_seconds=600,\n",
        "    lang_weights={\"english\":0.78,\"hindi\":0.20,\"punjabi\":0.02},\n",
        "    gap_sec_range=(0.2, 0.8),\n",
        "    overlap_prob=0.10,\n",
        "    overlap_sec_range=(0.5, 1.5),\n",
        "    speakers_per_session=(3,4),\n",
        "    min_clip_sec=0.5, max_clip_sec=20.0,\n",
        "    bind_key_by_lang={\"hindi\":\"client_id\",\"punjabi\":\"client_id\",\"english\":\"spk_proxy\"},\n",
        "    out_dir=f\"{BASE}/sessions_prop\",\n",
        "    scenario_name=\"sessions_prop\",\n",
        "    seed=125\n",
        ")\n",
        "\n",
        "# 4) High-overlap stress (~20%)\n",
        "cfg_overlap = GenConfig(\n",
        "    session_seconds=600,\n",
        "    lang_weights={\"english\":1,\"hindi\":1,\"punjabi\":1},\n",
        "    gap_sec_range=(0.0, 0.3),             # less gap\n",
        "    overlap_prob=0.40,                     # more overlaps\n",
        "    overlap_sec_range=(0.7, 2.0),\n",
        "    speakers_per_session=(3,4),\n",
        "    min_clip_sec=0.5, max_clip_sec=15.0,   # slightly shorter to pack more\n",
        "    bind_key_by_lang={\"hindi\":\"client_id\",\"punjabi\":\"client_id\",\"english\":\"spk_proxy\"},\n",
        "    out_dir=f\"{BASE}/sessions_high_overlap\",\n",
        "    scenario_name=\"sessions_high_overlap\",\n",
        "    seed=126\n",
        ")\n",
        "\n",
        "# 5) Code-switch heavy (alternating languages)\n",
        "cfg_codesw = GenConfig(\n",
        "    session_seconds=600,\n",
        "    lang_weights={\"english\":1,\"hindi\":1,\"punjabi\":1},  # equal total per session\n",
        "    gap_sec_range=(0.2, 0.8),\n",
        "    overlap_prob=0.10,\n",
        "    overlap_sec_range=(0.5, 1.5),\n",
        "    speakers_per_session=(3,4),\n",
        "    min_clip_sec=0.5, max_clip_sec=15.0,\n",
        "    bind_key_by_lang={\"hindi\":\"client_id\",\"punjabi\":\"client_id\",\"english\":\"spk_proxy\"},\n",
        "    force_code_switch=True,\n",
        "    code_switch_prob=0.6,\n",
        "    out_dir=f\"{BASE}/sessions_codeswitch\",\n",
        "    scenario_name=\"sessions_codeswitch\",\n",
        "    seed=127\n",
        ")\n",
        "\n",
        "# ---- Choose which to generate (uncomment to run) ----\n",
        "# SessionGenerator(MANIFEST, cfg_bal).generate(n_sessions=20)\n",
        "# SessionGenerator(MANIFEST, cfg_smooth).generate(n_sessions=20)\n",
        "# SessionGenerator(MANIFEST, cfg_prop).generate(n_sessions=20)\n",
        "SessionGenerator(MANIFEST, cfg_overlap).generate(n_sessions=20)\n",
        "SessionGenerator(MANIFEST, cfg_codesw).generate(n_sessions=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkJGmCR-3ZYS",
        "outputId": "0cd522aa-8577-4476-c4e6-01972c53bc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ sessions_high_overlap 001: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_001.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_001.jsonl with 134 segments\n",
            "✅ sessions_high_overlap 002: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_002.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_002.jsonl with 161 segments\n",
            "✅ sessions_high_overlap 003: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_003.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_003.jsonl with 140 segments\n",
            "✅ sessions_high_overlap 004: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_004.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_004.jsonl with 139 segments\n",
            "✅ sessions_high_overlap 005: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_005.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_005.jsonl with 145 segments\n",
            "✅ sessions_high_overlap 006: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_006.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_006.jsonl with 137 segments\n",
            "✅ sessions_high_overlap 007: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_007.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_007.jsonl with 132 segments\n",
            "✅ sessions_high_overlap 008: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_008.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_008.jsonl with 142 segments\n",
            "✅ sessions_high_overlap 009: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_009.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_009.jsonl with 133 segments\n",
            "✅ sessions_high_overlap 010: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_010.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_010.jsonl with 116 segments\n",
            "✅ sessions_high_overlap 011: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_011.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_011.jsonl with 133 segments\n",
            "✅ sessions_high_overlap 012: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_012.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_012.jsonl with 126 segments\n",
            "✅ sessions_high_overlap 013: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_013.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_013.jsonl with 132 segments\n",
            "✅ sessions_high_overlap 014: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_014.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_014.jsonl with 136 segments\n",
            "✅ sessions_high_overlap 015: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_015.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_015.jsonl with 132 segments\n",
            "✅ sessions_high_overlap 016: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_016.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_016.jsonl with 138 segments\n",
            "✅ sessions_high_overlap 017: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_017.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_017.jsonl with 141 segments\n",
            "✅ sessions_high_overlap 018: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_018.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_018.jsonl with 122 segments\n",
            "✅ sessions_high_overlap 019: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_019.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_019.jsonl with 133 segments\n",
            "✅ sessions_high_overlap 020: wrote /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_020.wav & /content/drive/MyDrive/problem_statement_6/sessions_high_overlap/session_020.jsonl with 138 segments\n",
            "\n",
            "Done. Sessions created: 20/20 → /content/drive/MyDrive/problem_statement_6/sessions_high_overlap\n",
            "✅ sessions_codeswitch 001: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_001.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_001.jsonl with 117 segments\n",
            "✅ sessions_codeswitch 002: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_002.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_002.jsonl with 119 segments\n",
            "✅ sessions_codeswitch 003: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_003.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_003.jsonl with 123 segments\n",
            "✅ sessions_codeswitch 004: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_004.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_004.jsonl with 134 segments\n",
            "✅ sessions_codeswitch 005: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_005.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_005.jsonl with 115 segments\n",
            "✅ sessions_codeswitch 006: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_006.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_006.jsonl with 117 segments\n",
            "✅ sessions_codeswitch 007: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_007.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_007.jsonl with 125 segments\n",
            "✅ sessions_codeswitch 008: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_008.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_008.jsonl with 110 segments\n",
            "✅ sessions_codeswitch 009: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_009.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_009.jsonl with 116 segments\n",
            "✅ sessions_codeswitch 010: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_010.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_010.jsonl with 113 segments\n",
            "✅ sessions_codeswitch 011: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_011.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_011.jsonl with 117 segments\n",
            "✅ sessions_codeswitch 012: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_012.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_012.jsonl with 107 segments\n",
            "✅ sessions_codeswitch 013: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_013.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_013.jsonl with 103 segments\n",
            "✅ sessions_codeswitch 014: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_014.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_014.jsonl with 110 segments\n",
            "✅ sessions_codeswitch 015: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_015.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_015.jsonl with 111 segments\n",
            "✅ sessions_codeswitch 016: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_016.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_016.jsonl with 116 segments\n",
            "✅ sessions_codeswitch 017: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_017.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_017.jsonl with 114 segments\n",
            "✅ sessions_codeswitch 018: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_018.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_018.jsonl with 117 segments\n",
            "✅ sessions_codeswitch 019: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_019.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_019.jsonl with 114 segments\n",
            "✅ sessions_codeswitch 020: wrote /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_020.wav & /content/drive/MyDrive/problem_statement_6/sessions_codeswitch/session_020.jsonl with 126 segments\n",
            "\n",
            "Done. Sessions created: 20/20 → /content/drive/MyDrive/problem_statement_6/sessions_codeswitch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, collections\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/problem_statement_6\"\n",
        "SCENARIOS = {\n",
        "    \"balanced\":   f\"{BASE}/sessions_balanced\",\n",
        "    \"smooth\":     f\"{BASE}/sessions_smooth\",\n",
        "    \"high_overlap\": f\"{BASE}/sessions_high_overlap\",\n",
        "    \"codeswitch\": f\"{BASE}/sessions_codeswitch\"\n",
        "}\n",
        "\n",
        "for name, folder in SCENARIOS.items():\n",
        "    if not os.path.isdir(folder):\n",
        "        print(f\"⚠️ Missing folder {folder}, skipping…\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n=== Language mix for {name} ===\")\n",
        "    lang_totals = collections.Counter()\n",
        "    n_sessions = 0\n",
        "\n",
        "    for f in sorted(os.listdir(folder)):\n",
        "        if not f.endswith(\".jsonl\"):\n",
        "            continue\n",
        "        n_sessions += 1\n",
        "        lang_counts = collections.Counter()\n",
        "        total = 0.0\n",
        "        with open(os.path.join(folder, f), \"r\", encoding=\"utf-8\") as fh:\n",
        "            for line in fh:\n",
        "                rec = json.loads(line)\n",
        "                lang = str(rec.get(\"language\",\"\")).lower()\n",
        "                dur = float(rec.get(\"end_ts\",0)) - float(rec.get(\"start_ts\",0))\n",
        "                if dur <= 0:\n",
        "                    continue\n",
        "                lang_counts[lang] += dur\n",
        "                lang_totals[lang] += dur\n",
        "                total += dur\n",
        "        if total > 0:\n",
        "            mix = {k: round(100*v/total,1) for k,v in lang_counts.items()}\n",
        "            print(f\" {f}: {mix}\")\n",
        "\n",
        "    total_all = sum(lang_totals.values())\n",
        "    if total_all > 0:\n",
        "        mix_all = {k: round(100*v/total_all,1) for k,v in lang_totals.items()}\n",
        "        print(f\"\\n → Overall across {n_sessions} sessions: {mix_all}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qymeUYkVkuOD",
        "outputId": "0ebcdef3-af63-4c2f-b84a-3408d1cdae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Language mix for balanced ===\n",
            " session_001.jsonl: {'hindi': 30.1, 'english': 60.2, 'punjabi': 9.7}\n",
            " session_002.jsonl: {'english': 59.4, 'hindi': 31.1, 'punjabi': 9.5}\n",
            " session_003.jsonl: {'hindi': 29.3, 'english': 60.8, 'punjabi': 9.9}\n",
            " session_004.jsonl: {'hindi': 29.1, 'english': 61.2, 'punjabi': 9.7}\n",
            " session_005.jsonl: {'english': 60.4, 'hindi': 29.5, 'punjabi': 10.1}\n",
            " session_006.jsonl: {'hindi': 30.0, 'punjabi': 8.3, 'english': 61.7}\n",
            " session_007.jsonl: {'punjabi': 9.5, 'hindi': 31.0, 'english': 59.5}\n",
            " session_008.jsonl: {'english': 60.8, 'punjabi': 9.7, 'hindi': 29.5}\n",
            " session_009.jsonl: {'hindi': 28.1, 'english': 61.4, 'punjabi': 10.4}\n",
            " session_010.jsonl: {'english': 59.9, 'hindi': 30.9, 'punjabi': 9.2}\n",
            " session_011.jsonl: {'english': 58.1, 'hindi': 31.4, 'punjabi': 10.5}\n",
            " session_012.jsonl: {'hindi': 29.3, 'punjabi': 9.6, 'english': 61.0}\n",
            " session_013.jsonl: {'english': 60.9, 'hindi': 30.2, 'punjabi': 8.9}\n",
            " session_014.jsonl: {'english': 61.6, 'hindi': 28.8, 'punjabi': 9.6}\n",
            " session_015.jsonl: {'english': 60.7, 'hindi': 30.3, 'punjabi': 9.1}\n",
            " session_016.jsonl: {'hindi': 29.6, 'english': 60.7, 'punjabi': 9.7}\n",
            " session_017.jsonl: {'english': 61.2, 'hindi': 31.0, 'punjabi': 7.8}\n",
            " session_018.jsonl: {'english': 61.3, 'hindi': 30.7, 'punjabi': 8.0}\n",
            " session_019.jsonl: {'english': 62.7, 'punjabi': 10.1, 'hindi': 27.2}\n",
            " session_020.jsonl: {'english': 61.0, 'hindi': 30.0, 'punjabi': 9.0}\n",
            "\n",
            " → Overall across 20 sessions: {'hindi': 29.9, 'english': 60.7, 'punjabi': 9.4}\n",
            "\n",
            "=== Language mix for smooth ===\n",
            " session_001.jsonl: {'english': 35.7, 'hindi': 31.3, 'punjabi': 33.0}\n",
            " session_002.jsonl: {'punjabi': 31.2, 'english': 35.6, 'hindi': 33.2}\n",
            " session_003.jsonl: {'english': 34.7, 'punjabi': 31.4, 'hindi': 33.9}\n",
            " session_004.jsonl: {'punjabi': 33.0, 'english': 33.8, 'hindi': 33.2}\n",
            " session_005.jsonl: {'punjabi': 31.5, 'hindi': 34.7, 'english': 33.8}\n",
            " session_006.jsonl: {'hindi': 34.9, 'english': 33.6, 'punjabi': 31.4}\n",
            " session_007.jsonl: {'hindi': 33.7, 'english': 34.5, 'punjabi': 31.8}\n",
            " session_008.jsonl: {'hindi': 34.4, 'punjabi': 31.5, 'english': 34.1}\n",
            " session_009.jsonl: {'english': 34.1, 'hindi': 34.3, 'punjabi': 31.6}\n",
            " session_010.jsonl: {'hindi': 32.1, 'punjabi': 34.2, 'english': 33.7}\n",
            " session_011.jsonl: {'english': 36.3, 'hindi': 34.5, 'punjabi': 29.1}\n",
            " session_012.jsonl: {'hindi': 31.9, 'english': 35.7, 'punjabi': 32.4}\n",
            " session_013.jsonl: {'hindi': 33.4, 'punjabi': 32.5, 'english': 34.1}\n",
            " session_014.jsonl: {'english': 35.3, 'punjabi': 31.9, 'hindi': 32.8}\n",
            " session_015.jsonl: {'hindi': 32.7, 'english': 35.0, 'punjabi': 32.3}\n",
            " session_016.jsonl: {'hindi': 34.6, 'punjabi': 29.8, 'english': 35.6}\n",
            " session_017.jsonl: {'hindi': 34.1, 'punjabi': 31.7, 'english': 34.2}\n",
            " session_018.jsonl: {'english': 35.0, 'hindi': 32.2, 'punjabi': 32.8}\n",
            " session_019.jsonl: {'punjabi': 30.9, 'hindi': 33.5, 'english': 35.6}\n",
            " session_020.jsonl: {'hindi': 33.3, 'english': 33.9, 'punjabi': 32.8}\n",
            "\n",
            " → Overall across 20 sessions: {'english': 34.7, 'hindi': 33.4, 'punjabi': 31.8}\n",
            "\n",
            "=== Language mix for high_overlap ===\n",
            " session_001.jsonl: {'hindi': 75.4, 'punjabi': 24.6}\n",
            " session_002.jsonl: {'punjabi': 85.2, 'english': 7.5, 'hindi': 7.3}\n",
            " session_003.jsonl: {'hindi': 24.2, 'english': 75.8}\n",
            " session_004.jsonl: {'hindi': 38.0, 'english': 62.0}\n",
            " session_005.jsonl: {'english': 64.1, 'hindi': 35.9}\n",
            " session_006.jsonl: {'hindi': 100.0}\n",
            " session_007.jsonl: {'english': 100.0}\n",
            " session_008.jsonl: {'english': 67.3, 'hindi': 32.7}\n",
            " session_009.jsonl: {'english': 60.8, 'hindi': 39.2}\n",
            " session_010.jsonl: {'english': 76.7, 'hindi': 23.3}\n",
            " session_011.jsonl: {'english': 100.0}\n",
            " session_012.jsonl: {'english': 57.1, 'hindi': 42.9}\n",
            " session_013.jsonl: {'english': 66.0, 'hindi': 34.0}\n",
            " session_014.jsonl: {'hindi': 29.5, 'english': 70.5}\n",
            " session_015.jsonl: {'english': 100.0}\n",
            " session_016.jsonl: {'hindi': 75.2, 'english': 24.8}\n",
            " session_017.jsonl: {'english': 100.0}\n",
            " session_018.jsonl: {'english': 74.0, 'hindi': 26.0}\n",
            " session_019.jsonl: {'english': 100.0}\n",
            " session_020.jsonl: {'english': 100.0}\n",
            "\n",
            " → Overall across 20 sessions: {'hindi': 29.2, 'punjabi': 5.5, 'english': 65.3}\n",
            "\n",
            "=== Language mix for codeswitch ===\n",
            " session_001.jsonl: {'hindi': 31.8, 'english': 36.5, 'punjabi': 31.6}\n",
            " session_002.jsonl: {'english': 32.7, 'punjabi': 34.0, 'hindi': 33.3}\n",
            " session_003.jsonl: {'hindi': 33.9, 'english': 33.7, 'punjabi': 32.5}\n",
            " session_004.jsonl: {'english': 35.2, 'hindi': 33.2, 'punjabi': 31.6}\n",
            " session_005.jsonl: {'hindi': 50.9, 'english': 49.1}\n",
            " session_006.jsonl: {'hindi': 49.2, 'english': 50.8}\n",
            " session_007.jsonl: {'english': 51.1, 'hindi': 48.9}\n",
            " session_008.jsonl: {'english': 53.2, 'hindi': 46.8}\n",
            " session_009.jsonl: {'english': 52.7, 'hindi': 47.3}\n",
            " session_010.jsonl: {'english': 48.8, 'hindi': 51.2}\n",
            " session_011.jsonl: {'hindi': 42.7, 'english': 57.3}\n",
            " session_012.jsonl: {'hindi': 50.1, 'english': 49.9}\n",
            " session_013.jsonl: {'hindi': 51.8, 'english': 48.2}\n",
            " session_014.jsonl: {'english': 49.3, 'hindi': 50.7}\n",
            " session_015.jsonl: {'english': 50.6, 'hindi': 49.4}\n",
            " session_016.jsonl: {'hindi': 53.3, 'english': 46.7}\n",
            " session_017.jsonl: {'english': 47.5, 'hindi': 52.5}\n",
            " session_018.jsonl: {'hindi': 49.9, 'english': 50.1}\n",
            " session_019.jsonl: {'hindi': 51.3, 'english': 48.7}\n",
            " session_020.jsonl: {'english': 45.2, 'hindi': 54.8}\n",
            "\n",
            " → Overall across 20 sessions: {'hindi': 46.7, 'english': 46.9, 'punjabi': 6.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmClQ39nlNHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}